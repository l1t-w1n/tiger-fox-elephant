2025-02-01 22:35:22,095 INFO: Configuration:
2025-02-01 22:35:22,095 INFO: IMG_SIZE: 224, BATCH_SIZE: 64, LR: 0.001, EPOCHS: 15
2025-02-01 22:35:22,096 INFO: Device: cuda
2025-02-01 22:35:22,212 INFO:  Target class: fox, Train samples: 9098, Val samples: 2600
2025-02-01 22:35:22,213 INFO: Data augmentation pipeline:
2025-02-01 22:35:22,213 INFO: ToPILImage()
2025-02-01 22:35:22,213 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 22:35:22,214 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 22:35:22,214 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 22:35:22,214 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 22:35:22,214 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 22:35:22,215 INFO: ToTensor()
2025-02-01 22:35:22,215 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 22:35:22,215 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 22:35:22,215 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 22:35:22,216 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 22:35:22,313 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 22:35:47,833 INFO: Epoch [1/15] Train Loss: 0.6762, Train Acc: 0.5841 | Val Loss: 0.6807, Val Acc: 0.5696 | LR: 0.001
2025-02-01 22:36:05,894 INFO: Configuration:
2025-02-01 22:36:05,895 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 15
2025-02-01 22:36:05,895 INFO: Device: cuda
2025-02-01 22:36:06,011 INFO:  Target class: fox, Train samples: 9098, Val samples: 2600
2025-02-01 22:36:06,011 INFO: Data augmentation pipeline:
2025-02-01 22:36:06,011 INFO: ToPILImage()
2025-02-01 22:36:06,012 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 22:36:06,012 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 22:36:06,012 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 22:36:06,012 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 22:36:06,012 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 22:36:06,012 INFO: ToTensor()
2025-02-01 22:36:06,012 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 22:36:06,013 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 22:36:06,013 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 22:36:06,013 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 22:36:06,167 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 22:36:32,130 INFO: Epoch [1/15] Train Loss: 0.6767, Train Acc: 0.5834 | Val Loss: 0.6731, Val Acc: 0.5727 | LR: 0.001
2025-02-01 22:36:57,476 INFO: Epoch [2/15] Train Loss: 0.6639, Train Acc: 0.6056 | Val Loss: 0.6657, Val Acc: 0.6019 | LR: 0.001
2025-02-01 22:37:22,797 INFO: Epoch [3/15] Train Loss: 0.6547, Train Acc: 0.6233 | Val Loss: 0.6522, Val Acc: 0.6169 | LR: 0.001
2025-02-01 22:37:47,966 INFO: Epoch [4/15] Train Loss: 0.6486, Train Acc: 0.6299 | Val Loss: 0.6463, Val Acc: 0.6392 | LR: 0.0001
2025-02-01 22:38:13,037 INFO: Epoch [5/15] Train Loss: 0.6428, Train Acc: 0.6394 | Val Loss: 0.6383, Val Acc: 0.6392 | LR: 0.0001
2025-02-01 22:38:38,550 INFO: Epoch [6/15] Train Loss: 0.6406, Train Acc: 0.6459 | Val Loss: 0.6414, Val Acc: 0.6338 | LR: 0.0001
2025-02-01 22:39:03,499 INFO: Epoch [7/15] Train Loss: 0.6373, Train Acc: 0.6496 | Val Loss: 0.6351, Val Acc: 0.6508 | LR: 1e-05
2025-02-01 22:44:02,372 INFO: Configuration:
2025-02-01 22:44:02,373 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 15
2025-02-01 22:44:02,373 INFO: Device: cuda
2025-02-01 22:44:02,488 INFO:  Target class: fox, Train samples: 9098, Val samples: 2600
2025-02-01 22:44:02,489 INFO: Data augmentation pipeline:
2025-02-01 22:44:02,489 INFO: ToPILImage()
2025-02-01 22:44:02,489 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 22:44:02,490 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 22:44:02,490 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 22:44:02,490 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 22:44:02,490 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 22:44:02,491 INFO: ToTensor()
2025-02-01 22:44:02,491 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 22:44:02,491 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 22:44:02,492 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 22:44:02,492 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 22:44:02,588 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 22:44:28,184 INFO: Epoch [1/15] Train Loss: 0.6767, Train Acc: 0.5834 | Val Loss: 0.6731, Val Acc: 0.5727 | LR: 0.001
2025-02-01 22:44:53,138 INFO: Epoch [2/15] Train Loss: 0.6639, Train Acc: 0.6056 | Val Loss: 0.6657, Val Acc: 0.6019 | LR: 0.001
2025-02-01 22:45:18,118 INFO: Epoch [3/15] Train Loss: 0.6547, Train Acc: 0.6233 | Val Loss: 0.6522, Val Acc: 0.6169 | LR: 0.001
2025-02-01 22:45:43,013 INFO: Epoch [4/15] Train Loss: 0.6495, Train Acc: 0.6310 | Val Loss: 0.6841, Val Acc: 0.5858 | LR: 0.001
2025-02-01 22:46:08,131 INFO: Epoch [5/15] Train Loss: 0.6393, Train Acc: 0.6457 | Val Loss: 0.6429, Val Acc: 0.6354 | LR: 0.001
2025-02-01 22:46:33,242 INFO: Epoch [6/15] Train Loss: 0.6322, Train Acc: 0.6525 | Val Loss: 0.6216, Val Acc: 0.6696 | LR: 0.001
2025-02-01 22:46:58,364 INFO: Epoch [7/15] Train Loss: 0.6192, Train Acc: 0.6741 | Val Loss: 0.6029, Val Acc: 0.6908 | LR: 0.001
2025-02-01 22:47:23,471 INFO: Epoch [8/15] Train Loss: 0.6070, Train Acc: 0.6818 | Val Loss: 0.5979, Val Acc: 0.6796 | LR: 0.001
2025-02-01 22:47:48,544 INFO: Epoch [9/15] Train Loss: 0.6009, Train Acc: 0.6862 | Val Loss: 0.6577, Val Acc: 0.6200 | LR: 0.001
2025-02-01 22:48:13,679 INFO: Epoch [10/15] Train Loss: 0.5862, Train Acc: 0.6940 | Val Loss: 0.5880, Val Acc: 0.6942 | LR: 0.001
2025-02-01 22:48:38,465 INFO: Epoch [11/15] Train Loss: 0.5839, Train Acc: 0.6915 | Val Loss: 0.6645, Val Acc: 0.6335 | LR: 0.001
2025-02-01 22:49:03,479 INFO: Epoch [12/15] Train Loss: 0.5756, Train Acc: 0.7076 | Val Loss: 0.5760, Val Acc: 0.7146 | LR: 0.001
2025-02-01 22:49:28,488 INFO: Epoch [13/15] Train Loss: 0.5671, Train Acc: 0.7120 | Val Loss: 0.5453, Val Acc: 0.7254 | LR: 0.001
2025-02-01 22:49:53,622 INFO: Epoch [14/15] Train Loss: 0.5588, Train Acc: 0.7202 | Val Loss: 0.5620, Val Acc: 0.7200 | LR: 0.001
2025-02-01 22:50:18,946 INFO: Epoch [15/15] Train Loss: 0.5513, Train Acc: 0.7250 | Val Loss: 0.5461, Val Acc: 0.7300 | LR: 0.001
2025-02-01 22:50:18,951 INFO: Saved model and history to:
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_model.pth
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_history.npz
2025-02-01 22:59:23,075 INFO: Configuration:
2025-02-01 22:59:23,076 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 15
2025-02-01 22:59:23,076 INFO: Device: cuda
2025-02-01 22:59:23,198 INFO:  Target class: fox, Train samples: 9098, Val samples: 2600
2025-02-01 22:59:23,199 INFO: Data augmentation pipeline:
2025-02-01 22:59:23,199 INFO: ToPILImage()
2025-02-01 22:59:23,199 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 22:59:23,199 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 22:59:23,199 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 22:59:23,200 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 22:59:23,200 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 22:59:23,200 INFO: ToTensor()
2025-02-01 22:59:23,200 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 22:59:23,200 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 22:59:23,200 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 22:59:23,200 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 22:59:45,690 INFO: Configuration:
2025-02-01 22:59:45,691 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 15
2025-02-01 22:59:45,691 INFO: Device: cuda
2025-02-01 22:59:45,808 INFO:  Target class: fox, Train samples: 9098, Val samples: 2600
2025-02-01 22:59:45,808 INFO: Data augmentation pipeline:
2025-02-01 22:59:45,809 INFO: ToPILImage()
2025-02-01 22:59:45,809 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 22:59:45,809 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 22:59:45,809 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 22:59:45,809 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 22:59:45,809 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 22:59:45,809 INFO: ToTensor()
2025-02-01 22:59:45,810 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 22:59:45,810 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 22:59:45,810 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 22:59:45,810 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 22:59:45,972 INFO: Loaded model from /media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_model.pth
2025-02-01 22:59:45,973 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 23:00:11,571 INFO: Epoch [1/15] Train Loss: 0.5420, Train Acc: 0.7311 | Val Loss: 0.5302, Val Acc: 0.7392 | LR: 0.001
2025-02-01 23:00:37,280 INFO: Epoch [2/15] Train Loss: 0.5384, Train Acc: 0.7320 | Val Loss: 0.5745, Val Acc: 0.7015 | LR: 0.001
2025-02-01 23:01:02,605 INFO: Epoch [3/15] Train Loss: 0.5323, Train Acc: 0.7406 | Val Loss: 0.5297, Val Acc: 0.7438 | LR: 0.001
2025-02-01 23:01:28,034 INFO: Epoch [4/15] Train Loss: 0.5343, Train Acc: 0.7386 | Val Loss: 0.5019, Val Acc: 0.7608 | LR: 0.001
2025-02-01 23:01:53,280 INFO: Epoch [5/15] Train Loss: 0.5253, Train Acc: 0.7439 | Val Loss: 0.5062, Val Acc: 0.7515 | LR: 0.001
2025-02-01 23:02:18,549 INFO: Epoch [6/15] Train Loss: 0.5267, Train Acc: 0.7437 | Val Loss: 0.4959, Val Acc: 0.7612 | LR: 0.001
2025-02-01 23:02:43,887 INFO: Epoch [7/15] Train Loss: 0.5281, Train Acc: 0.7458 | Val Loss: 0.5047, Val Acc: 0.7573 | LR: 0.001
2025-02-01 23:04:12,726 INFO: Configuration:
2025-02-01 23:04:12,727 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 15
2025-02-01 23:04:12,727 INFO: Device: cuda
2025-02-01 23:04:12,844 INFO:  Target class: fox, Train samples: 9098, Val samples: 2600
2025-02-01 23:04:12,845 INFO: Data augmentation pipeline:
2025-02-01 23:04:12,845 INFO: ToPILImage()
2025-02-01 23:04:12,845 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 23:04:12,846 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 23:04:12,846 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 23:04:12,846 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 23:04:12,847 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 23:04:12,847 INFO: ToTensor()
2025-02-01 23:04:12,847 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 23:04:12,847 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 23:04:12,848 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 23:04:12,848 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 23:04:12,947 INFO: Loaded model from /media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_model.pth
2025-02-01 23:04:12,947 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 23:04:38,790 INFO: Epoch [1/15] Train Loss: 0.5420, Train Acc: 0.7311 | Val Loss: 0.5302, Val Acc: 0.7392 | LR: 0.001
2025-02-01 23:04:38,795 INFO: New best model saved at epoch 1 with Val Loss: 0.5302
2025-02-01 23:05:04,412 INFO: Epoch [2/15] Train Loss: 0.5384, Train Acc: 0.7320 | Val Loss: 0.5745, Val Acc: 0.7015 | LR: 0.001
2025-02-01 23:05:29,682 INFO: Epoch [3/15] Train Loss: 0.5323, Train Acc: 0.7406 | Val Loss: 0.5297, Val Acc: 0.7438 | LR: 0.001
2025-02-01 23:05:29,817 INFO: New best model saved at epoch 3 with Val Loss: 0.5297
2025-02-01 23:05:55,408 INFO: Epoch [4/15] Train Loss: 0.5343, Train Acc: 0.7386 | Val Loss: 0.5019, Val Acc: 0.7608 | LR: 0.001
2025-02-01 23:05:55,554 INFO: New best model saved at epoch 4 with Val Loss: 0.5019
2025-02-01 23:06:20,806 INFO: Epoch [5/15] Train Loss: 0.5253, Train Acc: 0.7439 | Val Loss: 0.5062, Val Acc: 0.7515 | LR: 0.001
2025-02-01 23:06:46,190 INFO: Epoch [6/15] Train Loss: 0.5267, Train Acc: 0.7437 | Val Loss: 0.4959, Val Acc: 0.7612 | LR: 0.001
2025-02-01 23:06:46,326 INFO: New best model saved at epoch 6 with Val Loss: 0.4959
2025-02-01 23:07:11,633 INFO: Epoch [7/15] Train Loss: 0.5281, Train Acc: 0.7458 | Val Loss: 0.5047, Val Acc: 0.7573 | LR: 0.001
2025-02-01 23:07:37,035 INFO: Epoch [8/15] Train Loss: 0.5109, Train Acc: 0.7598 | Val Loss: 0.5710, Val Acc: 0.7115 | LR: 0.001
2025-02-01 23:08:02,537 INFO: Epoch [9/15] Train Loss: 0.5115, Train Acc: 0.7520 | Val Loss: 0.5662, Val Acc: 0.6973 | LR: 0.001
2025-02-01 23:08:27,875 INFO: Epoch [10/15] Train Loss: 0.5001, Train Acc: 0.7632 | Val Loss: 0.4990, Val Acc: 0.7615 | LR: 0.001
2025-02-01 23:08:53,420 INFO: Epoch [11/15] Train Loss: 0.4859, Train Acc: 0.7765 | Val Loss: 0.4647, Val Acc: 0.7869 | LR: 0.0001
2025-02-01 23:08:53,424 INFO: New best model saved at epoch 11 with Val Loss: 0.4647
2025-02-01 23:09:18,791 INFO: Epoch [12/15] Train Loss: 0.4749, Train Acc: 0.7807 | Val Loss: 0.4538, Val Acc: 0.7962 | LR: 0.0001
2025-02-01 23:09:18,794 INFO: New best model saved at epoch 12 with Val Loss: 0.4538
2025-02-01 23:09:44,153 INFO: Epoch [13/15] Train Loss: 0.4698, Train Acc: 0.7832 | Val Loss: 0.4499, Val Acc: 0.7946 | LR: 0.0001
2025-02-01 23:09:44,156 INFO: New best model saved at epoch 13 with Val Loss: 0.4499
2025-02-01 23:10:09,294 INFO: Epoch [14/15] Train Loss: 0.4671, Train Acc: 0.7872 | Val Loss: 0.4434, Val Acc: 0.8073 | LR: 0.0001
2025-02-01 23:10:09,298 INFO: New best model saved at epoch 14 with Val Loss: 0.4434
2025-02-01 23:10:34,447 INFO: Epoch [15/15] Train Loss: 0.4655, Train Acc: 0.7863 | Val Loss: 0.4453, Val Acc: 0.7942 | LR: 0.0001
2025-02-01 23:10:34,451 INFO: Saved model and history to:
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_model.pth
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_history.npz
2025-02-01 23:13:03,907 INFO: Configuration:
2025-02-01 23:13:03,908 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 30
2025-02-01 23:13:03,908 INFO: Device: cuda
2025-02-01 23:13:09,409 INFO:  Target class: tiger, Train samples: 9766, Val samples: 2790
2025-02-01 23:13:09,409 INFO: Data augmentation pipeline:
2025-02-01 23:13:09,410 INFO: ToPILImage()
2025-02-01 23:13:09,410 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 23:13:09,410 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 23:13:09,410 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 23:13:09,411 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 23:13:09,411 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 23:13:09,411 INFO: ToTensor()
2025-02-01 23:13:09,412 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 23:13:09,412 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 23:13:09,412 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 23:13:09,412 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 23:13:09,504 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 23:13:44,680 INFO: Configuration:
2025-02-01 23:13:44,680 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 30
2025-02-01 23:13:44,681 INFO: Device: cuda
2025-02-01 23:13:48,364 INFO:  Target class: tiger, Train samples: 9766, Val samples: 2790
2025-02-01 23:13:48,364 INFO: Data augmentation pipeline:
2025-02-01 23:13:48,365 INFO: ToPILImage()
2025-02-01 23:13:48,365 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 23:13:48,365 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 23:13:48,365 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 23:13:48,365 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 23:13:48,365 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 23:13:48,366 INFO: ToTensor()
2025-02-01 23:13:48,366 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 23:13:48,366 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 23:13:48,366 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 23:13:48,366 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 23:13:48,540 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 23:14:16,374 INFO: Epoch [1/30] Train Loss: 0.5938, Train Acc: 0.6723 | Val Loss: 0.5604, Val Acc: 0.7061 | LR: 0.001
2025-02-01 23:14:16,380 INFO: New best model saved at epoch 1 with Val Loss: 0.5604
2025-02-01 23:14:43,301 INFO: Epoch [2/30] Train Loss: 0.5604, Train Acc: 0.7111 | Val Loss: 0.5507, Val Acc: 0.7100 | LR: 0.001
2025-02-01 23:14:43,304 INFO: New best model saved at epoch 2 with Val Loss: 0.5507
2025-02-01 23:15:09,898 INFO: Epoch [3/30] Train Loss: 0.5506, Train Acc: 0.7225 | Val Loss: 0.5499, Val Acc: 0.7240 | LR: 0.001
2025-02-01 23:15:09,901 INFO: New best model saved at epoch 3 with Val Loss: 0.5499
2025-02-01 23:15:36,837 INFO: Epoch [4/30] Train Loss: 0.5301, Train Acc: 0.7378 | Val Loss: 0.6722, Val Acc: 0.6566 | LR: 0.001
2025-02-01 23:16:03,754 INFO: Epoch [5/30] Train Loss: 0.5229, Train Acc: 0.7406 | Val Loss: 0.5165, Val Acc: 0.7326 | LR: 0.001
2025-02-01 23:16:03,758 INFO: New best model saved at epoch 5 with Val Loss: 0.5165
2025-02-01 23:16:31,059 INFO: Epoch [6/30] Train Loss: 0.5124, Train Acc: 0.7472 | Val Loss: 0.5015, Val Acc: 0.7609 | LR: 0.001
2025-02-01 23:16:31,062 INFO: New best model saved at epoch 6 with Val Loss: 0.5015
2025-02-01 23:16:57,849 INFO: Epoch [7/30] Train Loss: 0.5083, Train Acc: 0.7535 | Val Loss: 0.4846, Val Acc: 0.7692 | LR: 0.001
2025-02-01 23:16:57,852 INFO: New best model saved at epoch 7 with Val Loss: 0.4846
2025-02-01 23:17:25,351 INFO: Epoch [8/30] Train Loss: 0.5006, Train Acc: 0.7624 | Val Loss: 0.4924, Val Acc: 0.7552 | LR: 0.001
2025-02-01 23:17:52,536 INFO: Epoch [9/30] Train Loss: 0.4865, Train Acc: 0.7727 | Val Loss: 0.6077, Val Acc: 0.6935 | LR: 0.001
2025-02-01 23:18:19,894 INFO: Epoch [10/30] Train Loss: 0.4773, Train Acc: 0.7741 | Val Loss: 0.4919, Val Acc: 0.7527 | LR: 0.001
2025-02-01 23:18:46,823 INFO: Epoch [11/30] Train Loss: 0.4670, Train Acc: 0.7823 | Val Loss: 0.4743, Val Acc: 0.7735 | LR: 0.001
2025-02-01 23:18:46,826 INFO: New best model saved at epoch 11 with Val Loss: 0.4743
2025-02-01 23:19:13,797 INFO: Epoch [12/30] Train Loss: 0.4682, Train Acc: 0.7865 | Val Loss: 0.5027, Val Acc: 0.7631 | LR: 0.001
2025-02-01 23:19:41,248 INFO: Epoch [13/30] Train Loss: 0.4538, Train Acc: 0.7933 | Val Loss: 0.4589, Val Acc: 0.7986 | LR: 0.001
2025-02-01 23:19:41,252 INFO: New best model saved at epoch 13 with Val Loss: 0.4589
2025-02-01 23:20:07,985 INFO: Epoch [14/30] Train Loss: 0.4536, Train Acc: 0.7913 | Val Loss: 0.4834, Val Acc: 0.7659 | LR: 0.001
2025-02-01 23:20:34,832 INFO: Epoch [15/30] Train Loss: 0.4466, Train Acc: 0.7987 | Val Loss: 0.5605, Val Acc: 0.7097 | LR: 0.001
2025-02-01 23:21:01,540 INFO: Epoch [16/30] Train Loss: 0.4383, Train Acc: 0.8015 | Val Loss: 0.4492, Val Acc: 0.7885 | LR: 0.001
2025-02-01 23:21:01,543 INFO: New best model saved at epoch 16 with Val Loss: 0.4492
2025-02-01 23:21:28,420 INFO: Epoch [17/30] Train Loss: 0.4320, Train Acc: 0.8040 | Val Loss: 0.6197, Val Acc: 0.6670 | LR: 0.001
2025-02-01 23:21:55,223 INFO: Epoch [18/30] Train Loss: 0.4328, Train Acc: 0.8065 | Val Loss: 0.4741, Val Acc: 0.7656 | LR: 0.001
2025-02-01 23:22:22,234 INFO: Epoch [19/30] Train Loss: 0.4241, Train Acc: 0.8110 | Val Loss: 0.5583, Val Acc: 0.7072 | LR: 0.001
2025-02-01 23:22:48,994 INFO: Epoch [20/30] Train Loss: 0.4189, Train Acc: 0.8124 | Val Loss: 0.3875, Val Acc: 0.8391 | LR: 0.001
2025-02-01 23:22:48,997 INFO: New best model saved at epoch 20 with Val Loss: 0.3875
2025-02-01 23:23:15,697 INFO: Epoch [21/30] Train Loss: 0.4106, Train Acc: 0.8207 | Val Loss: 0.4971, Val Acc: 0.7706 | LR: 0.001
2025-02-01 23:23:42,363 INFO: Epoch [22/30] Train Loss: 0.4029, Train Acc: 0.8245 | Val Loss: 0.4341, Val Acc: 0.8115 | LR: 0.001
2025-02-01 23:24:09,335 INFO: Epoch [23/30] Train Loss: 0.3987, Train Acc: 0.8247 | Val Loss: 0.5123, Val Acc: 0.7620 | LR: 0.001
2025-02-01 23:24:35,992 INFO: Epoch [24/30] Train Loss: 0.4013, Train Acc: 0.8230 | Val Loss: 0.4122, Val Acc: 0.8201 | LR: 0.001
2025-02-01 23:25:02,952 INFO: Epoch [25/30] Train Loss: 0.3754, Train Acc: 0.8414 | Val Loss: 0.3510, Val Acc: 0.8606 | LR: 0.0001
2025-02-01 23:25:02,956 INFO: New best model saved at epoch 25 with Val Loss: 0.3510
2025-02-01 23:25:29,646 INFO: Epoch [26/30] Train Loss: 0.3581, Train Acc: 0.8523 | Val Loss: 0.3686, Val Acc: 0.8337 | LR: 0.0001
2025-02-01 23:25:56,347 INFO: Epoch [27/30] Train Loss: 0.3636, Train Acc: 0.8517 | Val Loss: 0.3609, Val Acc: 0.8581 | LR: 0.0001
2025-02-01 23:26:22,934 INFO: Epoch [28/30] Train Loss: 0.3570, Train Acc: 0.8474 | Val Loss: 0.3795, Val Acc: 0.8269 | LR: 0.0001
2025-02-01 23:26:49,697 INFO: Epoch [29/30] Train Loss: 0.3582, Train Acc: 0.8493 | Val Loss: 0.3459, Val Acc: 0.8656 | LR: 0.0001
2025-02-01 23:26:49,702 INFO: New best model saved at epoch 29 with Val Loss: 0.3459
2025-02-01 23:27:16,711 INFO: Epoch [30/30] Train Loss: 0.3553, Train Acc: 0.8519 | Val Loss: 0.3523, Val Acc: 0.8559 | LR: 0.0001
2025-02-01 23:27:16,715 INFO: Saved model and history to:
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/tiger_with_augmentation_model.pth
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/tiger_with_augmentation_history.npz
2025-02-01 23:28:39,986 INFO: Configuration:
2025-02-01 23:28:39,986 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 30
2025-02-01 23:28:39,987 INFO: Device: cuda
2025-02-01 23:29:02,684 INFO:  Target class: elephant, Train samples: 16852, Val samples: 4814
2025-02-01 23:29:02,684 INFO: Data augmentation pipeline:
2025-02-01 23:29:02,685 INFO: ToPILImage()
2025-02-01 23:29:02,685 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 23:29:02,685 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 23:29:02,685 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 23:29:02,685 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 23:29:02,686 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 23:29:02,686 INFO: ToTensor()
2025-02-01 23:29:02,686 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 23:29:02,686 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 23:29:02,686 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 23:29:02,687 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 23:29:02,902 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 23:29:49,203 INFO: Epoch [1/30] Train Loss: 0.6659, Train Acc: 0.6045 | Val Loss: 0.6598, Val Acc: 0.6103 | LR: 0.001
2025-02-01 23:36:49,519 INFO: Configuration:
2025-02-01 23:36:49,519 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 30
2025-02-01 23:36:49,520 INFO: Device: cuda
2025-02-01 23:36:54,992 INFO:  Target class: elephant, Train samples: 16852, Val samples: 4814
2025-02-01 23:36:54,993 INFO: Data augmentation pipeline:
2025-02-01 23:36:54,993 INFO: ToPILImage()
2025-02-01 23:36:54,993 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 23:36:54,993 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 23:36:54,994 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 23:36:54,994 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 23:36:54,994 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 23:36:54,994 INFO: ToTensor()
2025-02-01 23:36:54,994 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 23:36:54,995 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 23:36:54,995 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 23:36:54,995 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 23:36:55,193 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 23:37:40,656 INFO: Epoch [1/30] Train Loss: 0.6659, Train Acc: 0.6045 | Val Loss: 0.6598, Val Acc: 0.6103 | LR: 0.001
2025-02-01 23:37:40,661 INFO: New best model saved at epoch 1 with Val Loss: 0.6598
2025-02-01 23:38:25,901 INFO: Epoch [2/30] Train Loss: 0.6376, Train Acc: 0.6421 | Val Loss: 0.6305, Val Acc: 0.6423 | LR: 0.001
2025-02-01 23:38:25,903 INFO: New best model saved at epoch 2 with Val Loss: 0.6305
2025-02-01 23:39:10,781 INFO: Epoch [3/30] Train Loss: 0.6176, Train Acc: 0.6647 | Val Loss: 0.6016, Val Acc: 0.6894 | LR: 0.001
2025-02-01 23:39:10,786 INFO: New best model saved at epoch 3 with Val Loss: 0.6016
2025-02-01 23:39:55,857 INFO: Epoch [4/30] Train Loss: 0.5938, Train Acc: 0.6920 | Val Loss: 0.5774, Val Acc: 0.7050 | LR: 0.001
2025-02-01 23:39:55,860 INFO: New best model saved at epoch 4 with Val Loss: 0.5774
2025-02-01 23:40:40,884 INFO: Epoch [5/30] Train Loss: 0.5741, Train Acc: 0.7050 | Val Loss: 0.5914, Val Acc: 0.6888 | LR: 0.001
2025-02-01 23:41:25,595 INFO: Epoch [6/30] Train Loss: 0.5664, Train Acc: 0.7137 | Val Loss: 0.5348, Val Acc: 0.7524 | LR: 0.001
2025-02-01 23:41:25,599 INFO: New best model saved at epoch 6 with Val Loss: 0.5348
2025-02-01 23:42:10,600 INFO: Epoch [7/30] Train Loss: 0.5502, Train Acc: 0.7298 | Val Loss: 0.5801, Val Acc: 0.6996 | LR: 0.001
2025-02-01 23:42:55,823 INFO: Epoch [8/30] Train Loss: 0.5333, Train Acc: 0.7407 | Val Loss: 0.9169, Val Acc: 0.5625 | LR: 0.001
2025-02-01 23:43:41,197 INFO: Epoch [9/30] Train Loss: 0.5278, Train Acc: 0.7403 | Val Loss: 0.5239, Val Acc: 0.7418 | LR: 0.001
2025-02-01 23:43:41,200 INFO: New best model saved at epoch 9 with Val Loss: 0.5239
2025-02-01 23:44:26,227 INFO: Epoch [10/30] Train Loss: 0.5298, Train Acc: 0.7467 | Val Loss: 0.5071, Val Acc: 0.7532 | LR: 0.001
2025-02-01 23:44:26,230 INFO: New best model saved at epoch 10 with Val Loss: 0.5071
2025-02-01 23:45:11,473 INFO: Epoch [11/30] Train Loss: 0.5060, Train Acc: 0.7587 | Val Loss: 0.4744, Val Acc: 0.7850 | LR: 0.001
2025-02-01 23:45:11,478 INFO: New best model saved at epoch 11 with Val Loss: 0.4744
2025-02-01 23:45:56,569 INFO: Epoch [12/30] Train Loss: 0.5077, Train Acc: 0.7585 | Val Loss: 0.4820, Val Acc: 0.7746 | LR: 0.001
2025-02-01 23:46:41,804 INFO: Epoch [13/30] Train Loss: 0.5002, Train Acc: 0.7673 | Val Loss: 0.5134, Val Acc: 0.7572 | LR: 0.001
2025-02-01 23:47:27,266 INFO: Epoch [14/30] Train Loss: 0.4910, Train Acc: 0.7694 | Val Loss: 0.5403, Val Acc: 0.7370 | LR: 0.001
2025-02-01 23:48:12,675 INFO: Epoch [15/30] Train Loss: 0.4874, Train Acc: 0.7712 | Val Loss: 0.4639, Val Acc: 0.7898 | LR: 0.001
2025-02-01 23:48:12,680 INFO: New best model saved at epoch 15 with Val Loss: 0.4639
2025-02-01 23:48:57,923 INFO: Epoch [16/30] Train Loss: 0.4883, Train Acc: 0.7716 | Val Loss: 0.4748, Val Acc: 0.7894 | LR: 0.001
2025-02-01 23:49:43,320 INFO: Epoch [17/30] Train Loss: 0.4753, Train Acc: 0.7798 | Val Loss: 0.4911, Val Acc: 0.7692 | LR: 0.001
2025-02-01 23:50:28,440 INFO: Epoch [18/30] Train Loss: 0.4809, Train Acc: 0.7783 | Val Loss: 0.4569, Val Acc: 0.8025 | LR: 0.001
2025-02-01 23:50:28,445 INFO: New best model saved at epoch 18 with Val Loss: 0.4569
2025-02-01 23:51:13,636 INFO: Epoch [19/30] Train Loss: 0.4718, Train Acc: 0.7801 | Val Loss: 0.4478, Val Acc: 0.7962 | LR: 0.001
2025-02-01 23:51:13,640 INFO: New best model saved at epoch 19 with Val Loss: 0.4478
2025-02-01 23:51:58,928 INFO: Epoch [20/30] Train Loss: 0.4708, Train Acc: 0.7849 | Val Loss: 0.5292, Val Acc: 0.7364 | LR: 0.001
2025-02-01 23:52:44,133 INFO: Epoch [21/30] Train Loss: 0.4654, Train Acc: 0.7863 | Val Loss: 0.5962, Val Acc: 0.7054 | LR: 0.001
2025-02-01 23:53:29,144 INFO: Epoch [22/30] Train Loss: 0.4666, Train Acc: 0.7859 | Val Loss: 0.4557, Val Acc: 0.7921 | LR: 0.001
2025-02-01 23:54:14,230 INFO: Epoch [23/30] Train Loss: 0.4575, Train Acc: 0.7917 | Val Loss: 0.4309, Val Acc: 0.8087 | LR: 0.001
2025-02-01 23:54:14,235 INFO: New best model saved at epoch 23 with Val Loss: 0.4309
2025-02-01 23:54:59,251 INFO: Epoch [24/30] Train Loss: 0.4566, Train Acc: 0.7945 | Val Loss: 0.4317, Val Acc: 0.8118 | LR: 0.001
2025-02-01 23:55:44,294 INFO: Epoch [25/30] Train Loss: 0.4481, Train Acc: 0.7965 | Val Loss: 0.4807, Val Acc: 0.7715 | LR: 0.001
2025-02-01 23:56:29,626 INFO: Epoch [26/30] Train Loss: 0.4485, Train Acc: 0.7954 | Val Loss: 0.4331, Val Acc: 0.8122 | LR: 0.001
2025-02-01 23:57:14,617 INFO: Epoch [27/30] Train Loss: 0.4386, Train Acc: 0.8050 | Val Loss: 0.4306, Val Acc: 0.8097 | LR: 0.001
2025-02-01 23:57:14,622 INFO: New best model saved at epoch 27 with Val Loss: 0.4306
2025-02-01 23:57:59,734 INFO: Epoch [28/30] Train Loss: 0.4389, Train Acc: 0.7998 | Val Loss: 0.4274, Val Acc: 0.8060 | LR: 0.001
2025-02-01 23:57:59,738 INFO: New best model saved at epoch 28 with Val Loss: 0.4274
2025-02-01 23:58:44,908 INFO: Epoch [29/30] Train Loss: 0.4383, Train Acc: 0.8040 | Val Loss: 0.4075, Val Acc: 0.8149 | LR: 0.001
2025-02-01 23:58:44,912 INFO: New best model saved at epoch 29 with Val Loss: 0.4075
2025-02-01 23:59:30,305 INFO: Epoch [30/30] Train Loss: 0.4298, Train Acc: 0.8117 | Val Loss: 0.4052, Val Acc: 0.8211 | LR: 0.001
2025-02-01 23:59:30,308 INFO: New best model saved at epoch 30 with Val Loss: 0.4052
2025-02-01 23:59:30,312 INFO: Saved model and history to:
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/elephant_with_augmentation_model.pth
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/elephant_with_augmentation_history.npz
2025-02-10 14:45:10,452 INFO: Configuration:
2025-02-10 14:45:10,453 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 30
2025-02-10 14:45:10,453 INFO: Device: cuda
2025-02-10 15:06:18,579 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.1646021..1.1605446].
2025-02-10 15:06:31,842 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.17312834..1.1573731].
2025-02-10 15:06:33,304 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.13820404..1.142947].
2025-02-10 15:06:35,090 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.16077559..1.177656].
2025-02-10 15:06:37,956 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.15317173..1.1225103].
2025-02-10 15:07:31,921 INFO: Configuration:
2025-02-10 15:07:31,922 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 30
2025-02-10 15:07:31,922 INFO: Device: cuda
2025-02-10 15:07:37,723 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.22680448..1.1676176].
2025-02-10 15:08:07,459 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.13646863..1.1608287].
2025-02-10 15:08:12,175 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.14225897..1.1512058].
2025-02-10 15:08:29,998 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.14703876..1.1604564].
2025-02-10 15:08:50,913 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.1938211..1.1447675].
2025-02-10 15:09:18,516 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.16271342..1.1563638].
2025-02-10 15:09:49,407 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.16694959..1.1613878].
2025-02-10 15:10:49,715 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.17604814..1.1513343].
2025-02-10 15:11:15,530 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.1737751..1.1458361].
2025-02-10 15:11:46,593 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.16560476..1.1439613].
2025-02-10 15:12:30,904 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.16635442..1.167391].
2025-02-10 15:12:43,250 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.16163477..1.1584076].
2025-02-10 15:13:04,916 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.18638074..1.174659].
2025-02-10 15:13:19,678 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.19562219..1.1758354].
2025-02-10 15:14:19,076 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.18238585..1.1889739].
2025-02-10 15:14:32,537 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.18390772..1.1819481].
2025-02-10 15:14:40,411 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.18164246..1.1704106].
2025-02-10 15:14:49,892 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.16979617..1.187624].
2025-02-10 15:14:57,369 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.17661586..1.1923493].
2025-02-10 15:15:07,415 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.1874303..1.1725974].
2025-02-10 15:31:19,060 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.19611578..1.1666168].
2025-02-10 15:31:58,860 INFO: Configuration:
2025-02-10 15:31:58,860 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 50
2025-02-10 15:31:58,860 INFO: Device: cuda
2025-02-10 15:32:24,252 INFO: Configuration:
2025-02-10 15:32:24,252 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 50
2025-02-10 15:32:24,253 INFO: Device: cuda
2025-02-10 15:32:53,811 INFO: Configuration:
2025-02-10 15:32:53,811 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 50
2025-02-10 15:32:53,811 INFO: Device: cuda
2025-02-10 15:32:53,965 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.16033961..1.1858907].
2025-02-10 15:33:03,643 INFO:  Target class: fox, Train samples: 9098, Val samples: 2600
2025-02-10 15:33:03,643 INFO: Data augmentation pipeline:
2025-02-10 15:33:03,644 INFO: ToPILImage()
2025-02-10 15:33:03,644 INFO: RandomHorizontalFlip(p=0.5)
2025-02-10 15:33:03,644 INFO: RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)
2025-02-10 15:33:03,644 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.05, 0.05), shear=[-3.0, 3.0])
2025-02-10 15:33:03,645 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-10 15:33:03,645 INFO: ToTensor()
2025-02-10 15:33:03,645 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-10 15:33:03,645 INFO: RandomApply(
    p=0.4
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-10 15:33:03,645 INFO: RandomErasing(p=0.2, scale=(0.01, 0.01), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-10 15:33:03,646 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-10 15:33:03,901 INFO: Loaded model from /media/Ittwin/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_model.pth
2025-02-10 15:33:03,902 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-10 15:33:36,707 INFO: Epoch [1/50] Train Loss: 0.4825, Train Acc: 0.7718 | Val Loss: 0.5165, Val Acc: 0.7435 | LR: 0.001
2025-02-10 15:33:36,711 INFO: New best model saved at epoch 1 with Val Loss: 0.5165
2025-02-10 15:34:07,447 INFO: Epoch [2/50] Train Loss: 0.4810, Train Acc: 0.7763 | Val Loss: 0.4703, Val Acc: 0.7888 | LR: 0.001
2025-02-10 15:34:07,450 INFO: New best model saved at epoch 2 with Val Loss: 0.4703
2025-02-10 15:34:38,274 INFO: Epoch [3/50] Train Loss: 0.4704, Train Acc: 0.7843 | Val Loss: 0.4592, Val Acc: 0.7838 | LR: 0.001
2025-02-10 15:34:38,279 INFO: New best model saved at epoch 3 with Val Loss: 0.4592
2025-02-10 15:35:09,640 INFO: Epoch [4/50] Train Loss: 0.4815, Train Acc: 0.7781 | Val Loss: 0.4669, Val Acc: 0.7865 | LR: 0.001
2025-02-10 15:35:40,247 INFO: Epoch [5/50] Train Loss: 0.4665, Train Acc: 0.7835 | Val Loss: 0.4545, Val Acc: 0.7896 | LR: 0.001
2025-02-10 15:35:40,250 INFO: New best model saved at epoch 5 with Val Loss: 0.4545
2025-02-10 15:36:11,020 INFO: Epoch [6/50] Train Loss: 0.4627, Train Acc: 0.7852 | Val Loss: 0.5049, Val Acc: 0.7665 | LR: 0.001
2025-02-10 15:36:41,902 INFO: Epoch [7/50] Train Loss: 0.4602, Train Acc: 0.7869 | Val Loss: 0.4610, Val Acc: 0.7758 | LR: 0.001
2025-02-10 15:37:12,433 INFO: Epoch [8/50] Train Loss: 0.4665, Train Acc: 0.7885 | Val Loss: 0.5746, Val Acc: 0.7081 | LR: 0.001
2025-02-10 15:37:42,919 INFO: Epoch [9/50] Train Loss: 0.4647, Train Acc: 0.7858 | Val Loss: 0.4557, Val Acc: 0.7862 | LR: 0.001
2025-02-10 15:38:13,744 INFO: Epoch [10/50] Train Loss: 0.4389, Train Acc: 0.8066 | Val Loss: 0.4040, Val Acc: 0.8246 | LR: 0.0001
2025-02-10 15:38:13,747 INFO: New best model saved at epoch 10 with Val Loss: 0.4040
2025-02-10 15:38:44,273 INFO: Epoch [11/50] Train Loss: 0.4296, Train Acc: 0.8113 | Val Loss: 0.4066, Val Acc: 0.8219 | LR: 0.0001
2025-02-10 15:39:15,221 INFO: Epoch [12/50] Train Loss: 0.4270, Train Acc: 0.8120 | Val Loss: 0.4043, Val Acc: 0.8181 | LR: 0.0001
2025-02-10 15:39:45,884 INFO: Epoch [13/50] Train Loss: 0.4255, Train Acc: 0.8133 | Val Loss: 0.4026, Val Acc: 0.8138 | LR: 0.0001
2025-02-10 15:39:45,887 INFO: New best model saved at epoch 13 with Val Loss: 0.4026
2025-02-10 15:40:16,506 INFO: Epoch [14/50] Train Loss: 0.4202, Train Acc: 0.8135 | Val Loss: 0.3941, Val Acc: 0.8296 | LR: 0.0001
2025-02-10 15:40:16,510 INFO: New best model saved at epoch 14 with Val Loss: 0.3941
2025-02-10 15:40:47,299 INFO: Epoch [15/50] Train Loss: 0.4203, Train Acc: 0.8163 | Val Loss: 0.4377, Val Acc: 0.7977 | LR: 0.0001
2025-02-10 15:41:17,805 INFO: Epoch [16/50] Train Loss: 0.4211, Train Acc: 0.8138 | Val Loss: 0.3920, Val Acc: 0.8304 | LR: 0.0001
2025-02-10 15:41:17,809 INFO: New best model saved at epoch 16 with Val Loss: 0.3920
2025-02-10 15:41:48,752 INFO: Epoch [17/50] Train Loss: 0.4148, Train Acc: 0.8160 | Val Loss: 0.4027, Val Acc: 0.8288 | LR: 0.0001
2025-02-10 15:42:19,397 INFO: Epoch [18/50] Train Loss: 0.4119, Train Acc: 0.8193 | Val Loss: 0.3948, Val Acc: 0.8273 | LR: 0.0001
2025-02-10 15:42:50,041 INFO: Epoch [19/50] Train Loss: 0.4126, Train Acc: 0.8219 | Val Loss: 0.3897, Val Acc: 0.8358 | LR: 0.0001
2025-02-10 15:42:50,045 INFO: New best model saved at epoch 19 with Val Loss: 0.3897
2025-02-10 15:43:20,929 INFO: Epoch [20/50] Train Loss: 0.4204, Train Acc: 0.8148 | Val Loss: 0.3888, Val Acc: 0.8288 | LR: 0.0001
2025-02-10 15:43:20,933 INFO: New best model saved at epoch 20 with Val Loss: 0.3888
2025-02-10 15:43:51,693 INFO: Epoch [21/50] Train Loss: 0.4079, Train Acc: 0.8203 | Val Loss: 0.3986, Val Acc: 0.8246 | LR: 0.0001
2025-02-10 15:44:22,570 INFO: Epoch [22/50] Train Loss: 0.4159, Train Acc: 0.8157 | Val Loss: 0.3847, Val Acc: 0.8327 | LR: 0.0001
2025-02-10 15:44:22,573 INFO: New best model saved at epoch 22 with Val Loss: 0.3847
2025-02-10 15:44:53,577 INFO: Epoch [23/50] Train Loss: 0.4178, Train Acc: 0.8151 | Val Loss: 0.3817, Val Acc: 0.8358 | LR: 0.0001
2025-02-10 15:44:53,581 INFO: New best model saved at epoch 23 with Val Loss: 0.3817
2025-02-10 15:45:24,495 INFO: Epoch [24/50] Train Loss: 0.4024, Train Acc: 0.8231 | Val Loss: 0.3909, Val Acc: 0.8338 | LR: 0.0001
2025-02-10 15:45:55,274 INFO: Epoch [25/50] Train Loss: 0.4178, Train Acc: 0.8147 | Val Loss: 0.3925, Val Acc: 0.8335 | LR: 0.0001
2025-02-10 15:46:25,998 INFO: Epoch [26/50] Train Loss: 0.4121, Train Acc: 0.8211 | Val Loss: 0.3950, Val Acc: 0.8288 | LR: 0.0001
2025-02-10 15:46:56,777 INFO: Epoch [27/50] Train Loss: 0.4057, Train Acc: 0.8195 | Val Loss: 0.3806, Val Acc: 0.8373 | LR: 0.0001
2025-02-10 15:46:56,780 INFO: New best model saved at epoch 27 with Val Loss: 0.3806
2025-02-10 15:47:27,580 INFO: Epoch [28/50] Train Loss: 0.4100, Train Acc: 0.8177 | Val Loss: 0.3862, Val Acc: 0.8281 | LR: 0.0001
2025-02-10 15:47:58,123 INFO: Epoch [29/50] Train Loss: 0.4054, Train Acc: 0.8222 | Val Loss: 0.3884, Val Acc: 0.8338 | LR: 0.0001
2025-02-10 15:48:28,774 INFO: Epoch [30/50] Train Loss: 0.4066, Train Acc: 0.8222 | Val Loss: 0.4079, Val Acc: 0.8150 | LR: 0.0001
2025-02-10 15:48:59,510 INFO: Epoch [31/50] Train Loss: 0.4065, Train Acc: 0.8238 | Val Loss: 0.3914, Val Acc: 0.8288 | LR: 0.0001
2025-02-10 15:49:30,397 INFO: Epoch [32/50] Train Loss: 0.4010, Train Acc: 0.8237 | Val Loss: 0.3736, Val Acc: 0.8377 | LR: 1e-05
2025-02-10 15:49:30,400 INFO: New best model saved at epoch 32 with Val Loss: 0.3736
2025-02-10 15:50:01,174 INFO: Epoch [33/50] Train Loss: 0.4070, Train Acc: 0.8241 | Val Loss: 0.3812, Val Acc: 0.8308 | LR: 1e-05
2025-02-10 15:50:31,654 INFO: Epoch [34/50] Train Loss: 0.4028, Train Acc: 0.8228 | Val Loss: 0.3705, Val Acc: 0.8392 | LR: 1e-05
2025-02-10 15:50:31,659 INFO: New best model saved at epoch 34 with Val Loss: 0.3705
2025-02-10 15:51:02,522 INFO: Epoch [35/50] Train Loss: 0.4063, Train Acc: 0.8238 | Val Loss: 0.3774, Val Acc: 0.8381 | LR: 1e-05
2025-02-10 15:51:33,050 INFO: Epoch [36/50] Train Loss: 0.3966, Train Acc: 0.8262 | Val Loss: 0.3826, Val Acc: 0.8346 | LR: 1e-05
2025-02-10 15:52:03,853 INFO: Epoch [37/50] Train Loss: 0.4024, Train Acc: 0.8264 | Val Loss: 0.3739, Val Acc: 0.8396 | LR: 1e-05
2025-02-10 15:52:34,651 INFO: Epoch [38/50] Train Loss: 0.4042, Train Acc: 0.8215 | Val Loss: 0.3795, Val Acc: 0.8400 | LR: 1e-05
2025-02-10 15:53:05,506 INFO: Epoch [39/50] Train Loss: 0.4031, Train Acc: 0.8240 | Val Loss: 0.3770, Val Acc: 0.8427 | LR: 1.0000000000000002e-06
2025-02-10 15:53:36,169 INFO: Epoch [40/50] Train Loss: 0.3999, Train Acc: 0.8251 | Val Loss: 0.3770, Val Acc: 0.8446 | LR: 1.0000000000000002e-06
2025-02-10 15:54:07,168 INFO: Epoch [41/50] Train Loss: 0.4025, Train Acc: 0.8239 | Val Loss: 0.3831, Val Acc: 0.8331 | LR: 1.0000000000000002e-06
2025-02-10 15:54:37,868 INFO: Epoch [42/50] Train Loss: 0.4081, Train Acc: 0.8209 | Val Loss: 0.3758, Val Acc: 0.8415 | LR: 1.0000000000000002e-06
2025-02-10 15:55:08,559 INFO: Epoch [43/50] Train Loss: 0.3982, Train Acc: 0.8215 | Val Loss: 0.3844, Val Acc: 0.8350 | LR: 1.0000000000000002e-07
2025-02-10 15:55:39,178 INFO: Epoch [44/50] Train Loss: 0.4030, Train Acc: 0.8264 | Val Loss: 0.3769, Val Acc: 0.8369 | LR: 1.0000000000000002e-07
2025-02-10 15:56:09,749 INFO: Epoch [45/50] Train Loss: 0.4031, Train Acc: 0.8223 | Val Loss: 0.3804, Val Acc: 0.8381 | LR: 1.0000000000000002e-07
2025-02-10 15:56:40,476 INFO: Epoch [46/50] Train Loss: 0.3975, Train Acc: 0.8249 | Val Loss: 0.3783, Val Acc: 0.8327 | LR: 1.0000000000000002e-07
2025-02-10 15:57:11,399 INFO: Epoch [47/50] Train Loss: 0.3987, Train Acc: 0.8267 | Val Loss: 0.3827, Val Acc: 0.8365 | LR: 1.0000000000000004e-08
2025-02-10 15:57:42,326 INFO: Epoch [48/50] Train Loss: 0.4033, Train Acc: 0.8220 | Val Loss: 0.3796, Val Acc: 0.8396 | LR: 1.0000000000000004e-08
2025-02-10 15:58:13,081 INFO: Epoch [49/50] Train Loss: 0.4013, Train Acc: 0.8256 | Val Loss: 0.3772, Val Acc: 0.8381 | LR: 1.0000000000000004e-08
2025-02-10 15:58:43,965 INFO: Epoch [50/50] Train Loss: 0.3991, Train Acc: 0.8231 | Val Loss: 0.3824, Val Acc: 0.8331 | LR: 1.0000000000000004e-08
2025-02-10 15:58:43,972 INFO: Saved model and history to:
/media/Ittwin/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_model.pth
/media/Ittwin/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_history.npz
2025-02-10 16:00:29,684 INFO: Configuration:
2025-02-10 16:00:29,685 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 35
2025-02-10 16:00:29,685 INFO: Device: cuda
2025-02-10 16:00:29,835 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.16033961..1.1858907].
2025-02-10 16:00:39,713 INFO:  Target class: tiger, Train samples: 9766, Val samples: 2790
2025-02-10 16:00:39,714 INFO: Data augmentation pipeline:
2025-02-10 16:00:39,714 INFO: ToPILImage()
2025-02-10 16:00:39,714 INFO: RandomHorizontalFlip(p=0.5)
2025-02-10 16:00:39,715 INFO: RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)
2025-02-10 16:00:39,715 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.05, 0.05), shear=[-3.0, 3.0])
2025-02-10 16:00:39,715 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-10 16:00:39,716 INFO: ToTensor()
2025-02-10 16:00:39,716 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-10 16:00:39,716 INFO: RandomApply(
    p=0.4
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-10 16:00:39,717 INFO: RandomErasing(p=0.2, scale=(0.01, 0.01), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-10 16:00:39,717 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-10 16:00:39,926 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-10 16:01:13,844 INFO: Epoch [1/35] Train Loss: 0.5867, Train Acc: 0.6869 | Val Loss: 0.5676, Val Acc: 0.7154 | LR: 0.001
2025-02-10 16:01:13,850 INFO: New best model saved at epoch 1 with Val Loss: 0.5676
2025-02-10 16:01:47,297 INFO: Epoch [2/35] Train Loss: 0.5545, Train Acc: 0.7192 | Val Loss: 0.5233, Val Acc: 0.7437 | LR: 0.001
2025-02-10 16:01:47,302 INFO: New best model saved at epoch 2 with Val Loss: 0.5233
2025-02-10 16:02:20,352 INFO: Epoch [3/35] Train Loss: 0.5371, Train Acc: 0.7262 | Val Loss: 0.6029, Val Acc: 0.6595 | LR: 0.001
2025-02-10 16:02:53,350 INFO: Epoch [4/35] Train Loss: 0.5244, Train Acc: 0.7382 | Val Loss: 0.5082, Val Acc: 0.7566 | LR: 0.001
2025-02-10 16:02:53,353 INFO: New best model saved at epoch 4 with Val Loss: 0.5082
2025-02-10 16:03:26,227 INFO: Epoch [5/35] Train Loss: 0.5026, Train Acc: 0.7536 | Val Loss: 0.4931, Val Acc: 0.7796 | LR: 0.001
2025-02-10 16:03:26,231 INFO: New best model saved at epoch 5 with Val Loss: 0.4931
2025-02-10 16:03:59,460 INFO: Epoch [6/35] Train Loss: 0.4771, Train Acc: 0.7740 | Val Loss: 0.4536, Val Acc: 0.7896 | LR: 0.001
2025-02-10 16:03:59,464 INFO: New best model saved at epoch 6 with Val Loss: 0.4536
2025-02-10 16:04:32,483 INFO: Epoch [7/35] Train Loss: 0.4588, Train Acc: 0.7893 | Val Loss: 0.6660, Val Acc: 0.6319 | LR: 0.001
2025-02-10 16:05:05,426 INFO: Epoch [8/35] Train Loss: 0.4520, Train Acc: 0.7945 | Val Loss: 0.4400, Val Acc: 0.7982 | LR: 0.001
2025-02-10 16:05:05,430 INFO: New best model saved at epoch 8 with Val Loss: 0.4400
2025-02-10 16:05:38,330 INFO: Epoch [9/35] Train Loss: 0.4402, Train Acc: 0.8018 | Val Loss: 0.4491, Val Acc: 0.7789 | LR: 0.001
2025-02-10 16:06:11,232 INFO: Epoch [10/35] Train Loss: 0.4308, Train Acc: 0.8039 | Val Loss: 0.5429, Val Acc: 0.7384 | LR: 0.001
2025-02-10 16:06:44,379 INFO: Epoch [11/35] Train Loss: 0.4170, Train Acc: 0.8137 | Val Loss: 0.4731, Val Acc: 0.7566 | LR: 0.001
2025-02-10 16:07:17,201 INFO: Epoch [12/35] Train Loss: 0.4139, Train Acc: 0.8176 | Val Loss: 0.8420, Val Acc: 0.6577 | LR: 0.001
2025-02-10 16:07:50,093 INFO: Epoch [13/35] Train Loss: 0.3868, Train Acc: 0.8366 | Val Loss: 0.3573, Val Acc: 0.8523 | LR: 0.0001
2025-02-10 16:07:50,097 INFO: New best model saved at epoch 13 with Val Loss: 0.3573
2025-02-10 16:08:23,138 INFO: Epoch [14/35] Train Loss: 0.3753, Train Acc: 0.8432 | Val Loss: 0.3618, Val Acc: 0.8477 | LR: 0.0001
2025-02-10 16:08:56,316 INFO: Epoch [15/35] Train Loss: 0.3710, Train Acc: 0.8439 | Val Loss: 0.3580, Val Acc: 0.8473 | LR: 0.0001
2025-02-10 16:09:29,329 INFO: Epoch [16/35] Train Loss: 0.3750, Train Acc: 0.8422 | Val Loss: 0.3468, Val Acc: 0.8591 | LR: 0.0001
2025-02-10 16:09:29,333 INFO: New best model saved at epoch 16 with Val Loss: 0.3468
2025-02-10 16:10:02,334 INFO: Epoch [17/35] Train Loss: 0.3719, Train Acc: 0.8448 | Val Loss: 0.3546, Val Acc: 0.8491 | LR: 0.0001
2025-02-10 16:10:35,368 INFO: Epoch [18/35] Train Loss: 0.3641, Train Acc: 0.8472 | Val Loss: 0.3513, Val Acc: 0.8556 | LR: 0.0001
2025-02-10 16:11:08,672 INFO: Epoch [19/35] Train Loss: 0.3673, Train Acc: 0.8468 | Val Loss: 0.3589, Val Acc: 0.8448 | LR: 0.0001
2025-02-10 16:11:41,575 INFO: Epoch [20/35] Train Loss: 0.3570, Train Acc: 0.8510 | Val Loss: 0.3451, Val Acc: 0.8645 | LR: 0.0001
2025-02-10 16:11:41,580 INFO: New best model saved at epoch 20 with Val Loss: 0.3451
2025-02-10 16:12:14,613 INFO: Epoch [21/35] Train Loss: 0.3655, Train Acc: 0.8461 | Val Loss: 0.3587, Val Acc: 0.8545 | LR: 0.0001
2025-02-10 16:12:47,474 INFO: Epoch [22/35] Train Loss: 0.3655, Train Acc: 0.8484 | Val Loss: 0.3566, Val Acc: 0.8581 | LR: 0.0001
2025-02-10 16:13:20,775 INFO: Epoch [23/35] Train Loss: 0.3615, Train Acc: 0.8498 | Val Loss: 0.3418, Val Acc: 0.8656 | LR: 0.0001
2025-02-10 16:13:20,779 INFO: New best model saved at epoch 23 with Val Loss: 0.3418
2025-02-10 16:13:53,923 INFO: Epoch [24/35] Train Loss: 0.3578, Train Acc: 0.8477 | Val Loss: 0.3420, Val Acc: 0.8659 | LR: 0.0001
2025-02-10 16:14:26,893 INFO: Epoch [25/35] Train Loss: 0.3535, Train Acc: 0.8551 | Val Loss: 0.3415, Val Acc: 0.8602 | LR: 0.0001
2025-02-10 16:14:26,898 INFO: New best model saved at epoch 25 with Val Loss: 0.3415
2025-02-10 16:14:59,722 INFO: Epoch [26/35] Train Loss: 0.3568, Train Acc: 0.8490 | Val Loss: 0.4018, Val Acc: 0.8237 | LR: 0.0001
2025-02-10 16:15:32,814 INFO: Epoch [27/35] Train Loss: 0.3567, Train Acc: 0.8498 | Val Loss: 0.3884, Val Acc: 0.8351 | LR: 0.0001
2025-02-10 16:16:06,081 INFO: Epoch [28/35] Train Loss: 0.3515, Train Acc: 0.8534 | Val Loss: 0.3316, Val Acc: 0.8602 | LR: 0.0001
2025-02-10 16:16:06,085 INFO: New best model saved at epoch 28 with Val Loss: 0.3316
2025-02-10 16:16:39,380 INFO: Epoch [29/35] Train Loss: 0.3551, Train Acc: 0.8531 | Val Loss: 0.3428, Val Acc: 0.8591 | LR: 0.0001
2025-02-10 16:17:12,395 INFO: Epoch [30/35] Train Loss: 0.3548, Train Acc: 0.8493 | Val Loss: 0.3654, Val Acc: 0.8452 | LR: 0.0001
2025-02-10 16:17:45,613 INFO: Epoch [31/35] Train Loss: 0.3548, Train Acc: 0.8543 | Val Loss: 0.3390, Val Acc: 0.8627 | LR: 0.0001
2025-02-10 16:18:18,641 INFO: Epoch [32/35] Train Loss: 0.3540, Train Acc: 0.8515 | Val Loss: 0.3231, Val Acc: 0.8710 | LR: 0.0001
2025-02-10 16:18:18,646 INFO: New best model saved at epoch 32 with Val Loss: 0.3231
2025-02-10 16:18:52,010 INFO: Epoch [33/35] Train Loss: 0.3470, Train Acc: 0.8551 | Val Loss: 0.3524, Val Acc: 0.8563 | LR: 0.0001
2025-02-10 16:19:25,337 INFO: Epoch [34/35] Train Loss: 0.3485, Train Acc: 0.8582 | Val Loss: 0.3407, Val Acc: 0.8577 | LR: 0.0001
2025-02-10 16:19:58,507 INFO: Epoch [35/35] Train Loss: 0.3517, Train Acc: 0.8550 | Val Loss: 0.3443, Val Acc: 0.8548 | LR: 0.0001
2025-02-10 16:19:58,512 INFO: Saved model and history to:
/media/Ittwin/E/_uni/DL_Project/tiger-fox-elephant/weights/tiger_with_augmentation_model.pth
/media/Ittwin/E/_uni/DL_Project/tiger-fox-elephant/weights/tiger_with_augmentation_history.npz
2025-02-10 16:19:58,672 INFO:  Target class: elephant, Train samples: 16852, Val samples: 4814
2025-02-10 16:19:58,672 INFO: Data augmentation pipeline:
2025-02-10 16:19:58,672 INFO: ToPILImage()
2025-02-10 16:19:58,672 INFO: RandomHorizontalFlip(p=0.5)
2025-02-10 16:19:58,673 INFO: RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)
2025-02-10 16:19:58,673 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.05, 0.05), shear=[-3.0, 3.0])
2025-02-10 16:19:58,673 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-10 16:19:58,673 INFO: ToTensor()
2025-02-10 16:19:58,673 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-10 16:19:58,674 INFO: RandomApply(
    p=0.4
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-10 16:19:58,674 INFO: RandomErasing(p=0.2, scale=(0.01, 0.01), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-10 16:19:58,674 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-10 16:19:58,677 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-10 16:20:56,817 INFO: Epoch [1/35] Train Loss: 0.6556, Train Acc: 0.6171 | Val Loss: 0.6640, Val Acc: 0.6120 | LR: 0.001
2025-02-10 16:20:56,820 INFO: New best model saved at epoch 1 with Val Loss: 0.6640
2025-02-10 16:23:57,596 INFO: Configuration:
2025-02-10 16:23:57,596 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 70
2025-02-10 16:23:57,597 INFO: Device: cuda
2025-02-10 16:23:57,748 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.16033961..1.1858907].
2025-02-10 16:24:06,528 INFO:  Target class: tiger, Train samples: 9766, Val samples: 2790
2025-02-10 16:24:06,528 INFO: Data augmentation pipeline:
2025-02-10 16:24:06,529 INFO: ToPILImage()
2025-02-10 16:24:06,529 INFO: RandomHorizontalFlip(p=0.5)
2025-02-10 16:24:06,529 INFO: RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)
2025-02-10 16:24:06,530 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.05, 0.05), shear=[-3.0, 3.0])
2025-02-10 16:24:06,530 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-10 16:24:06,530 INFO: ToTensor()
2025-02-10 16:24:06,531 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-10 16:24:06,531 INFO: RandomApply(
    p=0.4
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-10 16:24:06,531 INFO: RandomErasing(p=0.2, scale=(0.01, 0.01), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-10 16:24:06,531 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-10 16:24:06,683 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-10 16:24:40,491 INFO: Epoch [1/70] Train Loss: 0.5867, Train Acc: 0.6869 | Val Loss: 0.5676, Val Acc: 0.7154 | LR: 0.001
2025-02-10 16:24:40,498 INFO: New best model saved at epoch 1 with Val Loss: 0.5676
2025-02-10 16:25:13,814 INFO: Epoch [2/70] Train Loss: 0.5545, Train Acc: 0.7189 | Val Loss: 0.5232, Val Acc: 0.7448 | LR: 0.001
2025-02-10 16:25:13,817 INFO: New best model saved at epoch 2 with Val Loss: 0.5232
2025-02-10 16:25:47,165 INFO: Epoch [3/70] Train Loss: 0.5373, Train Acc: 0.7261 | Val Loss: 0.6046, Val Acc: 0.6602 | LR: 0.001
2025-02-10 16:26:20,346 INFO: Epoch [4/70] Train Loss: 0.5230, Train Acc: 0.7379 | Val Loss: 0.5184, Val Acc: 0.7502 | LR: 0.001
2025-02-10 16:26:20,350 INFO: New best model saved at epoch 4 with Val Loss: 0.5184
2025-02-10 16:26:53,656 INFO: Epoch [5/70] Train Loss: 0.5024, Train Acc: 0.7533 | Val Loss: 0.4794, Val Acc: 0.7771 | LR: 0.001
2025-02-10 16:26:53,660 INFO: New best model saved at epoch 5 with Val Loss: 0.4794
2025-02-10 16:27:26,912 INFO: Epoch [6/70] Train Loss: 0.4786, Train Acc: 0.7727 | Val Loss: 0.4592, Val Acc: 0.7871 | LR: 0.001
2025-02-10 16:27:26,915 INFO: New best model saved at epoch 6 with Val Loss: 0.4592
2025-02-10 16:28:00,178 INFO: Epoch [7/70] Train Loss: 0.4616, Train Acc: 0.7844 | Val Loss: 0.6163, Val Acc: 0.6566 | LR: 0.001
2025-02-10 16:28:33,039 INFO: Epoch [8/70] Train Loss: 0.4485, Train Acc: 0.7942 | Val Loss: 0.4946, Val Acc: 0.7487 | LR: 0.001
2025-02-10 16:29:06,395 INFO: Epoch [9/70] Train Loss: 0.4390, Train Acc: 0.8008 | Val Loss: 0.4614, Val Acc: 0.7634 | LR: 0.001
2025-02-10 16:29:39,777 INFO: Epoch [10/70] Train Loss: 0.4306, Train Acc: 0.8045 | Val Loss: 0.6288, Val Acc: 0.6975 | LR: 0.001
2025-02-10 16:30:12,675 INFO: Epoch [11/70] Train Loss: 0.4029, Train Acc: 0.8296 | Val Loss: 0.3823, Val Acc: 0.8448 | LR: 0.0001
2025-02-10 16:30:12,679 INFO: New best model saved at epoch 11 with Val Loss: 0.3823
2025-02-10 16:30:45,738 INFO: Epoch [12/70] Train Loss: 0.3914, Train Acc: 0.8321 | Val Loss: 0.3770, Val Acc: 0.8419 | LR: 0.0001
2025-02-10 16:30:45,741 INFO: New best model saved at epoch 12 with Val Loss: 0.3770
2025-02-10 16:31:18,849 INFO: Epoch [13/70] Train Loss: 0.3878, Train Acc: 0.8347 | Val Loss: 0.3685, Val Acc: 0.8416 | LR: 0.0001
2025-02-10 16:31:18,852 INFO: New best model saved at epoch 13 with Val Loss: 0.3685
2025-02-10 16:31:51,949 INFO: Epoch [14/70] Train Loss: 0.3821, Train Acc: 0.8384 | Val Loss: 0.3674, Val Acc: 0.8473 | LR: 0.0001
2025-02-10 16:31:51,953 INFO: New best model saved at epoch 14 with Val Loss: 0.3674
2025-02-10 16:32:25,097 INFO: Epoch [15/70] Train Loss: 0.3798, Train Acc: 0.8400 | Val Loss: 0.3688, Val Acc: 0.8444 | LR: 0.0001
2025-02-10 16:32:57,971 INFO: Epoch [16/70] Train Loss: 0.3843, Train Acc: 0.8365 | Val Loss: 0.3566, Val Acc: 0.8530 | LR: 0.0001
2025-02-10 16:32:57,975 INFO: New best model saved at epoch 16 with Val Loss: 0.3566
2025-02-10 16:33:31,110 INFO: Epoch [17/70] Train Loss: 0.3816, Train Acc: 0.8378 | Val Loss: 0.3615, Val Acc: 0.8513 | LR: 0.0001
2025-02-10 16:34:04,278 INFO: Epoch [18/70] Train Loss: 0.3726, Train Acc: 0.8417 | Val Loss: 0.3598, Val Acc: 0.8527 | LR: 0.0001
2025-02-10 16:34:37,369 INFO: Epoch [19/70] Train Loss: 0.3764, Train Acc: 0.8422 | Val Loss: 0.3683, Val Acc: 0.8416 | LR: 0.0001
2025-02-10 16:35:10,232 INFO: Epoch [20/70] Train Loss: 0.3666, Train Acc: 0.8456 | Val Loss: 0.3549, Val Acc: 0.8570 | LR: 0.0001
2025-02-10 16:35:10,235 INFO: New best model saved at epoch 20 with Val Loss: 0.3549
2025-02-10 16:35:43,316 INFO: Epoch [21/70] Train Loss: 0.3755, Train Acc: 0.8407 | Val Loss: 0.3651, Val Acc: 0.8523 | LR: 0.0001
2025-02-10 16:36:16,662 INFO: Epoch [22/70] Train Loss: 0.3743, Train Acc: 0.8419 | Val Loss: 0.3692, Val Acc: 0.8505 | LR: 0.0001
2025-02-10 16:36:49,769 INFO: Epoch [23/70] Train Loss: 0.3702, Train Acc: 0.8457 | Val Loss: 0.3531, Val Acc: 0.8588 | LR: 0.0001
2025-02-10 16:36:49,772 INFO: New best model saved at epoch 23 with Val Loss: 0.3531
2025-02-10 16:37:23,106 INFO: Epoch [24/70] Train Loss: 0.3661, Train Acc: 0.8454 | Val Loss: 0.3523, Val Acc: 0.8591 | LR: 0.0001
2025-02-10 16:37:23,110 INFO: New best model saved at epoch 24 with Val Loss: 0.3523
2025-02-10 16:37:56,411 INFO: Epoch [25/70] Train Loss: 0.3623, Train Acc: 0.8501 | Val Loss: 0.3477, Val Acc: 0.8548 | LR: 0.0001
2025-02-10 16:37:56,415 INFO: New best model saved at epoch 25 with Val Loss: 0.3477
2025-02-10 16:38:29,666 INFO: Epoch [26/70] Train Loss: 0.3656, Train Acc: 0.8456 | Val Loss: 0.4157, Val Acc: 0.8100 | LR: 0.0001
2025-02-10 16:39:02,786 INFO: Epoch [27/70] Train Loss: 0.3652, Train Acc: 0.8459 | Val Loss: 0.3928, Val Acc: 0.8294 | LR: 0.0001
2025-02-10 16:39:35,764 INFO: Epoch [28/70] Train Loss: 0.3602, Train Acc: 0.8471 | Val Loss: 0.3409, Val Acc: 0.8588 | LR: 0.0001
2025-02-10 16:39:35,766 INFO: New best model saved at epoch 28 with Val Loss: 0.3409
2025-02-10 16:40:08,888 INFO: Epoch [29/70] Train Loss: 0.3640, Train Acc: 0.8468 | Val Loss: 0.3548, Val Acc: 0.8516 | LR: 0.0001
2025-02-10 16:40:42,100 INFO: Epoch [30/70] Train Loss: 0.3629, Train Acc: 0.8448 | Val Loss: 0.3643, Val Acc: 0.8502 | LR: 0.0001
2025-02-10 16:41:15,135 INFO: Epoch [31/70] Train Loss: 0.3634, Train Acc: 0.8515 | Val Loss: 0.3445, Val Acc: 0.8627 | LR: 0.0001
2025-02-10 16:41:48,451 INFO: Epoch [32/70] Train Loss: 0.3618, Train Acc: 0.8489 | Val Loss: 0.3295, Val Acc: 0.8659 | LR: 0.0001
2025-02-10 16:41:48,456 INFO: New best model saved at epoch 32 with Val Loss: 0.3295
2025-02-10 16:42:21,670 INFO: Epoch [33/70] Train Loss: 0.3559, Train Acc: 0.8498 | Val Loss: 0.3607, Val Acc: 0.8491 | LR: 0.0001
2025-02-10 16:42:54,801 INFO: Epoch [34/70] Train Loss: 0.3569, Train Acc: 0.8535 | Val Loss: 0.3473, Val Acc: 0.8570 | LR: 0.0001
2025-02-10 16:43:28,133 INFO: Epoch [35/70] Train Loss: 0.3598, Train Acc: 0.8511 | Val Loss: 0.3463, Val Acc: 0.8552 | LR: 0.0001
2025-02-10 16:44:01,255 INFO: Epoch [36/70] Train Loss: 0.3548, Train Acc: 0.8527 | Val Loss: 0.3255, Val Acc: 0.8670 | LR: 0.0001
2025-02-10 16:44:01,259 INFO: New best model saved at epoch 36 with Val Loss: 0.3255
2025-02-10 16:44:34,536 INFO: Epoch [37/70] Train Loss: 0.3613, Train Acc: 0.8528 | Val Loss: 0.3285, Val Acc: 0.8695 | LR: 0.0001
2025-02-10 16:45:07,745 INFO: Epoch [38/70] Train Loss: 0.3587, Train Acc: 0.8495 | Val Loss: 0.3530, Val Acc: 0.8606 | LR: 0.0001
2025-02-10 16:45:40,952 INFO: Epoch [39/70] Train Loss: 0.3486, Train Acc: 0.8531 | Val Loss: 0.3356, Val Acc: 0.8634 | LR: 0.0001
2025-02-10 16:46:14,158 INFO: Epoch [40/70] Train Loss: 0.3509, Train Acc: 0.8544 | Val Loss: 0.3241, Val Acc: 0.8742 | LR: 0.0001
2025-02-10 16:46:14,163 INFO: New best model saved at epoch 40 with Val Loss: 0.3241
2025-02-10 16:46:47,342 INFO: Epoch [41/70] Train Loss: 0.3501, Train Acc: 0.8550 | Val Loss: 0.3310, Val Acc: 0.8599 | LR: 0.0001
2025-02-10 16:47:20,635 INFO: Epoch [42/70] Train Loss: 0.3426, Train Acc: 0.8597 | Val Loss: 0.3244, Val Acc: 0.8742 | LR: 0.0001
2025-02-10 16:47:53,917 INFO: Epoch [43/70] Train Loss: 0.3456, Train Acc: 0.8554 | Val Loss: 0.3298, Val Acc: 0.8710 | LR: 0.0001
2025-02-10 16:48:26,945 INFO: Epoch [44/70] Train Loss: 0.3462, Train Acc: 0.8573 | Val Loss: 0.3367, Val Acc: 0.8681 | LR: 0.0001
2025-02-10 16:49:00,631 INFO: Epoch [45/70] Train Loss: 0.3480, Train Acc: 0.8549 | Val Loss: 0.3225, Val Acc: 0.8720 | LR: 1e-05
2025-02-10 16:49:00,634 INFO: New best model saved at epoch 45 with Val Loss: 0.3225
2025-02-10 16:49:34,936 INFO: Epoch [46/70] Train Loss: 0.3407, Train Acc: 0.8623 | Val Loss: 0.3274, Val Acc: 0.8738 | LR: 1e-05
2025-02-10 16:50:08,717 INFO: Epoch [47/70] Train Loss: 0.3402, Train Acc: 0.8565 | Val Loss: 0.3195, Val Acc: 0.8738 | LR: 1e-05
2025-02-10 16:50:08,721 INFO: New best model saved at epoch 47 with Val Loss: 0.3195
2025-02-10 16:50:41,746 INFO: Epoch [48/70] Train Loss: 0.3408, Train Acc: 0.8576 | Val Loss: 0.3193, Val Acc: 0.8756 | LR: 1e-05
2025-02-10 16:50:41,750 INFO: New best model saved at epoch 48 with Val Loss: 0.3193
2025-02-10 16:51:14,764 INFO: Epoch [49/70] Train Loss: 0.3415, Train Acc: 0.8584 | Val Loss: 0.3174, Val Acc: 0.8781 | LR: 1e-05
2025-02-10 16:51:14,768 INFO: New best model saved at epoch 49 with Val Loss: 0.3174
2025-02-10 16:51:47,710 INFO: Epoch [50/70] Train Loss: 0.3384, Train Acc: 0.8620 | Val Loss: 0.3282, Val Acc: 0.8753 | LR: 1e-05
2025-02-10 16:52:20,770 INFO: Epoch [51/70] Train Loss: 0.3418, Train Acc: 0.8650 | Val Loss: 0.3215, Val Acc: 0.8756 | LR: 1e-05
2025-02-10 16:52:53,923 INFO: Epoch [52/70] Train Loss: 0.3419, Train Acc: 0.8599 | Val Loss: 0.3259, Val Acc: 0.8767 | LR: 1e-05
2025-02-10 16:53:27,155 INFO: Epoch [53/70] Train Loss: 0.3448, Train Acc: 0.8571 | Val Loss: 0.3125, Val Acc: 0.8767 | LR: 1e-05
2025-02-10 16:53:27,160 INFO: New best model saved at epoch 53 with Val Loss: 0.3125
2025-02-10 16:54:00,210 INFO: Epoch [54/70] Train Loss: 0.3419, Train Acc: 0.8599 | Val Loss: 0.3206, Val Acc: 0.8785 | LR: 1e-05
2025-02-10 16:54:33,607 INFO: Epoch [55/70] Train Loss: 0.3456, Train Acc: 0.8585 | Val Loss: 0.3233, Val Acc: 0.8756 | LR: 1e-05
2025-02-10 16:55:07,170 INFO: Epoch [56/70] Train Loss: 0.3400, Train Acc: 0.8586 | Val Loss: 0.3194, Val Acc: 0.8753 | LR: 1e-05
2025-02-10 16:55:40,425 INFO: Epoch [57/70] Train Loss: 0.3391, Train Acc: 0.8646 | Val Loss: 0.3234, Val Acc: 0.8785 | LR: 1e-05
2025-02-10 16:56:13,695 INFO: Epoch [58/70] Train Loss: 0.3409, Train Acc: 0.8576 | Val Loss: 0.3196, Val Acc: 0.8756 | LR: 1.0000000000000002e-06
2025-02-10 16:56:46,896 INFO: Epoch [59/70] Train Loss: 0.3411, Train Acc: 0.8579 | Val Loss: 0.3216, Val Acc: 0.8706 | LR: 1.0000000000000002e-06
2025-02-10 16:57:20,156 INFO: Epoch [60/70] Train Loss: 0.3401, Train Acc: 0.8597 | Val Loss: 0.3282, Val Acc: 0.8720 | LR: 1.0000000000000002e-06
2025-02-10 16:57:53,334 INFO: Epoch [61/70] Train Loss: 0.3414, Train Acc: 0.8579 | Val Loss: 0.3160, Val Acc: 0.8753 | LR: 1.0000000000000002e-06
2025-02-10 16:58:26,577 INFO: Epoch [62/70] Train Loss: 0.3346, Train Acc: 0.8620 | Val Loss: 0.3250, Val Acc: 0.8767 | LR: 1.0000000000000002e-07
2025-02-10 16:58:59,621 INFO: Epoch [63/70] Train Loss: 0.3398, Train Acc: 0.8610 | Val Loss: 0.3274, Val Acc: 0.8717 | LR: 1.0000000000000002e-07
2025-02-10 16:59:32,765 INFO: Epoch [64/70] Train Loss: 0.3371, Train Acc: 0.8642 | Val Loss: 0.3168, Val Acc: 0.8746 | LR: 1.0000000000000002e-07
2025-02-10 17:00:05,750 INFO: Epoch [65/70] Train Loss: 0.3372, Train Acc: 0.8597 | Val Loss: 0.3223, Val Acc: 0.8767 | LR: 1.0000000000000002e-07
2025-02-10 17:00:38,933 INFO: Epoch [66/70] Train Loss: 0.3411, Train Acc: 0.8609 | Val Loss: 0.3191, Val Acc: 0.8789 | LR: 1.0000000000000004e-08
2025-02-10 17:01:12,378 INFO: Epoch [67/70] Train Loss: 0.3383, Train Acc: 0.8612 | Val Loss: 0.3255, Val Acc: 0.8731 | LR: 1.0000000000000004e-08
2025-02-10 17:01:45,810 INFO: Epoch [68/70] Train Loss: 0.3348, Train Acc: 0.8666 | Val Loss: 0.3319, Val Acc: 0.8649 | LR: 1.0000000000000004e-08
2025-02-10 17:02:19,312 INFO: Epoch [69/70] Train Loss: 0.3449, Train Acc: 0.8580 | Val Loss: 0.3225, Val Acc: 0.8763 | LR: 1.0000000000000004e-08
2025-02-10 17:02:52,279 INFO: Epoch [70/70] Train Loss: 0.3412, Train Acc: 0.8606 | Val Loss: 0.3208, Val Acc: 0.8692 | LR: 1.0000000000000004e-08
2025-02-10 17:02:52,283 INFO: Saved model and history to:
/media/Ittwin/E/_uni/DL_Project/tiger-fox-elephant/weights/tiger_with_augmentation_model.pth
/media/Ittwin/E/_uni/DL_Project/tiger-fox-elephant/weights/tiger_with_augmentation_history.npz
2025-02-10 17:02:52,427 INFO:  Target class: elephant, Train samples: 16852, Val samples: 4814
2025-02-10 17:02:52,427 INFO: Data augmentation pipeline:
2025-02-10 17:02:52,427 INFO: ToPILImage()
2025-02-10 17:02:52,427 INFO: RandomHorizontalFlip(p=0.5)
2025-02-10 17:02:52,427 INFO: RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)
2025-02-10 17:02:52,428 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.05, 0.05), shear=[-3.0, 3.0])
2025-02-10 17:02:52,428 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-10 17:02:52,428 INFO: ToTensor()
2025-02-10 17:02:52,428 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-10 17:02:52,428 INFO: RandomApply(
    p=0.4
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-10 17:02:52,428 INFO: RandomErasing(p=0.2, scale=(0.01, 0.01), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-10 17:02:52,428 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-10 17:02:52,430 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-10 17:03:47,797 INFO: Epoch [1/70] Train Loss: 0.6518, Train Acc: 0.6200 | Val Loss: 0.6281, Val Acc: 0.6421 | LR: 0.001
2025-02-10 17:03:47,801 INFO: New best model saved at epoch 1 with Val Loss: 0.6281
2025-02-10 17:04:42,699 INFO: Epoch [2/70] Train Loss: 0.6208, Train Acc: 0.6592 | Val Loss: 0.5963, Val Acc: 0.6907 | LR: 0.001
2025-02-10 17:04:42,703 INFO: New best model saved at epoch 2 with Val Loss: 0.5963
2025-02-10 17:05:37,599 INFO: Epoch [3/70] Train Loss: 0.5808, Train Acc: 0.7037 | Val Loss: 0.6502, Val Acc: 0.6639 | LR: 0.001
2025-02-10 17:06:32,258 INFO: Epoch [4/70] Train Loss: 0.5620, Train Acc: 0.7184 | Val Loss: 0.5902, Val Acc: 0.6614 | LR: 0.001
2025-02-10 17:06:32,261 INFO: New best model saved at epoch 4 with Val Loss: 0.5902
2025-02-10 17:07:26,863 INFO: Epoch [5/70] Train Loss: 0.5423, Train Acc: 0.7384 | Val Loss: 0.6037, Val Acc: 0.6913 | LR: 0.001
2025-02-10 17:08:21,754 INFO: Epoch [6/70] Train Loss: 0.5185, Train Acc: 0.7501 | Val Loss: 0.4854, Val Acc: 0.7740 | LR: 0.001
2025-02-10 17:08:21,757 INFO: New best model saved at epoch 6 with Val Loss: 0.4854
2025-02-10 17:09:16,783 INFO: Epoch [7/70] Train Loss: 0.5093, Train Acc: 0.7582 | Val Loss: 0.5066, Val Acc: 0.7651 | LR: 0.001
2025-02-10 17:10:11,516 INFO: Epoch [8/70] Train Loss: 0.4992, Train Acc: 0.7651 | Val Loss: 0.5269, Val Acc: 0.7426 | LR: 0.001
2025-02-10 17:11:06,369 INFO: Epoch [9/70] Train Loss: 0.4962, Train Acc: 0.7679 | Val Loss: 0.7920, Val Acc: 0.5814 | LR: 0.001
2025-02-10 17:12:01,275 INFO: Epoch [10/70] Train Loss: 0.4866, Train Acc: 0.7749 | Val Loss: 0.4399, Val Acc: 0.8116 | LR: 0.001
2025-02-10 17:12:01,278 INFO: New best model saved at epoch 10 with Val Loss: 0.4399
2025-02-10 17:12:56,041 INFO: Epoch [11/70] Train Loss: 0.4788, Train Acc: 0.7785 | Val Loss: 0.9586, Val Acc: 0.5773 | LR: 0.001
2025-02-10 17:13:50,995 INFO: Epoch [12/70] Train Loss: 0.4697, Train Acc: 0.7830 | Val Loss: 0.5143, Val Acc: 0.7507 | LR: 0.001
2025-02-10 17:14:46,214 INFO: Epoch [13/70] Train Loss: 0.4634, Train Acc: 0.7898 | Val Loss: 0.4217, Val Acc: 0.8214 | LR: 0.001
2025-02-10 17:14:46,216 INFO: New best model saved at epoch 13 with Val Loss: 0.4217
2025-02-10 17:15:40,973 INFO: Epoch [14/70] Train Loss: 0.4628, Train Acc: 0.7889 | Val Loss: 0.4237, Val Acc: 0.8205 | LR: 0.001
2025-02-10 17:16:35,809 INFO: Epoch [15/70] Train Loss: 0.4605, Train Acc: 0.7918 | Val Loss: 0.6125, Val Acc: 0.6770 | LR: 0.001
2025-02-10 17:17:31,144 INFO: Epoch [16/70] Train Loss: 0.4526, Train Acc: 0.7989 | Val Loss: 0.5208, Val Acc: 0.7634 | LR: 0.001
2025-02-10 17:18:26,084 INFO: Epoch [17/70] Train Loss: 0.4531, Train Acc: 0.7962 | Val Loss: 0.4340, Val Acc: 0.8064 | LR: 0.001
2025-02-10 17:19:20,986 INFO: Epoch [18/70] Train Loss: 0.4245, Train Acc: 0.8163 | Val Loss: 0.3893, Val Acc: 0.8417 | LR: 0.0001
2025-02-10 17:19:20,990 INFO: New best model saved at epoch 18 with Val Loss: 0.3893
2025-02-10 17:20:15,941 INFO: Epoch [19/70] Train Loss: 0.4139, Train Acc: 0.8241 | Val Loss: 0.3870, Val Acc: 0.8475 | LR: 0.0001
2025-02-10 17:20:15,945 INFO: New best model saved at epoch 19 with Val Loss: 0.3870
2025-02-10 17:21:10,754 INFO: Epoch [20/70] Train Loss: 0.4140, Train Acc: 0.8225 | Val Loss: 0.3953, Val Acc: 0.8384 | LR: 0.0001
2025-02-10 17:22:05,617 INFO: Epoch [21/70] Train Loss: 0.4093, Train Acc: 0.8225 | Val Loss: 0.3859, Val Acc: 0.8432 | LR: 0.0001
2025-02-10 17:22:05,619 INFO: New best model saved at epoch 21 with Val Loss: 0.3859
2025-02-10 17:23:00,537 INFO: Epoch [22/70] Train Loss: 0.4085, Train Acc: 0.8241 | Val Loss: 0.3817, Val Acc: 0.8442 | LR: 0.0001
2025-02-10 17:23:00,541 INFO: New best model saved at epoch 22 with Val Loss: 0.3817
2025-02-10 17:23:55,355 INFO: Epoch [23/70] Train Loss: 0.4090, Train Acc: 0.8257 | Val Loss: 0.3864, Val Acc: 0.8386 | LR: 0.0001
2025-02-10 17:24:50,320 INFO: Epoch [24/70] Train Loss: 0.4067, Train Acc: 0.8253 | Val Loss: 0.3860, Val Acc: 0.8317 | LR: 0.0001
2025-02-10 17:25:45,210 INFO: Epoch [25/70] Train Loss: 0.4068, Train Acc: 0.8254 | Val Loss: 0.3834, Val Acc: 0.8398 | LR: 0.0001
2025-02-10 17:26:39,939 INFO: Epoch [26/70] Train Loss: 0.4069, Train Acc: 0.8243 | Val Loss: 0.3970, Val Acc: 0.8251 | LR: 0.0001
2025-02-10 17:27:34,732 INFO: Epoch [27/70] Train Loss: 0.3974, Train Acc: 0.8295 | Val Loss: 0.3740, Val Acc: 0.8477 | LR: 1e-05
2025-02-10 17:27:34,735 INFO: New best model saved at epoch 27 with Val Loss: 0.3740
2025-02-10 17:28:29,293 INFO: Epoch [28/70] Train Loss: 0.3999, Train Acc: 0.8324 | Val Loss: 0.3670, Val Acc: 0.8496 | LR: 1e-05
2025-02-10 17:28:29,298 INFO: New best model saved at epoch 28 with Val Loss: 0.3670
2025-02-10 17:29:24,208 INFO: Epoch [29/70] Train Loss: 0.3956, Train Acc: 0.8297 | Val Loss: 0.3744, Val Acc: 0.8506 | LR: 1e-05
2025-02-10 17:30:18,771 INFO: Epoch [30/70] Train Loss: 0.3974, Train Acc: 0.8298 | Val Loss: 0.3713, Val Acc: 0.8488 | LR: 1e-05
2025-02-10 17:31:13,524 INFO: Epoch [31/70] Train Loss: 0.3964, Train Acc: 0.8322 | Val Loss: 0.3733, Val Acc: 0.8492 | LR: 1e-05
2025-02-10 17:32:08,405 INFO: Epoch [32/70] Train Loss: 0.3953, Train Acc: 0.8294 | Val Loss: 0.3730, Val Acc: 0.8457 | LR: 1e-05
2025-02-10 17:33:03,166 INFO: Epoch [33/70] Train Loss: 0.3962, Train Acc: 0.8301 | Val Loss: 0.3713, Val Acc: 0.8504 | LR: 1.0000000000000002e-06
2025-02-10 17:33:57,937 INFO: Epoch [34/70] Train Loss: 0.4006, Train Acc: 0.8259 | Val Loss: 0.3678, Val Acc: 0.8506 | LR: 1.0000000000000002e-06
2025-02-10 17:34:52,926 INFO: Epoch [35/70] Train Loss: 0.3984, Train Acc: 0.8291 | Val Loss: 0.3697, Val Acc: 0.8500 | LR: 1.0000000000000002e-06
2025-02-10 17:35:48,104 INFO: Epoch [36/70] Train Loss: 0.3963, Train Acc: 0.8347 | Val Loss: 0.3744, Val Acc: 0.8525 | LR: 1.0000000000000002e-06
2025-02-10 17:36:42,855 INFO: Epoch [37/70] Train Loss: 0.3956, Train Acc: 0.8317 | Val Loss: 0.3768, Val Acc: 0.8490 | LR: 1.0000000000000002e-07
2025-02-10 17:37:37,670 INFO: Epoch [38/70] Train Loss: 0.3994, Train Acc: 0.8276 | Val Loss: 0.3704, Val Acc: 0.8504 | LR: 1.0000000000000002e-07
2025-02-10 17:38:32,594 INFO: Epoch [39/70] Train Loss: 0.3963, Train Acc: 0.8314 | Val Loss: 0.3724, Val Acc: 0.8490 | LR: 1.0000000000000002e-07
2025-02-10 17:39:27,681 INFO: Epoch [40/70] Train Loss: 0.3968, Train Acc: 0.8313 | Val Loss: 0.3684, Val Acc: 0.8479 | LR: 1.0000000000000002e-07
2025-02-10 17:40:22,463 INFO: Epoch [41/70] Train Loss: 0.3994, Train Acc: 0.8282 | Val Loss: 0.3750, Val Acc: 0.8444 | LR: 1.0000000000000004e-08
2025-02-10 17:41:17,040 INFO: Epoch [42/70] Train Loss: 0.4019, Train Acc: 0.8276 | Val Loss: 0.3722, Val Acc: 0.8488 | LR: 1.0000000000000004e-08
2025-02-10 17:42:12,056 INFO: Epoch [43/70] Train Loss: 0.3944, Train Acc: 0.8299 | Val Loss: 0.3760, Val Acc: 0.8475 | LR: 1.0000000000000004e-08
2025-02-10 17:43:07,196 INFO: Epoch [44/70] Train Loss: 0.3975, Train Acc: 0.8292 | Val Loss: 0.3727, Val Acc: 0.8500 | LR: 1.0000000000000004e-08
2025-02-10 17:44:01,892 INFO: Epoch [45/70] Train Loss: 0.3995, Train Acc: 0.8286 | Val Loss: 0.3705, Val Acc: 0.8519 | LR: 1.0000000000000004e-08
2025-02-10 17:44:56,889 INFO: Epoch [46/70] Train Loss: 0.3992, Train Acc: 0.8309 | Val Loss: 0.3740, Val Acc: 0.8490 | LR: 1.0000000000000004e-08
2025-02-10 17:45:51,859 INFO: Epoch [47/70] Train Loss: 0.4010, Train Acc: 0.8270 | Val Loss: 0.3687, Val Acc: 0.8533 | LR: 1.0000000000000004e-08
2025-02-10 17:46:46,673 INFO: Epoch [48/70] Train Loss: 0.3960, Train Acc: 0.8297 | Val Loss: 0.3719, Val Acc: 0.8494 | LR: 1.0000000000000004e-08
2025-02-10 17:47:41,479 INFO: Epoch [49/70] Train Loss: 0.3963, Train Acc: 0.8311 | Val Loss: 0.3729, Val Acc: 0.8521 | LR: 1.0000000000000004e-08
2025-02-10 17:48:36,499 INFO: Epoch [50/70] Train Loss: 0.3982, Train Acc: 0.8302 | Val Loss: 0.3677, Val Acc: 0.8529 | LR: 1.0000000000000004e-08
2025-02-10 17:49:32,768 INFO: Epoch [51/70] Train Loss: 0.3958, Train Acc: 0.8321 | Val Loss: 0.3715, Val Acc: 0.8504 | LR: 1.0000000000000004e-08
2025-02-10 17:50:29,540 INFO: Epoch [52/70] Train Loss: 0.3967, Train Acc: 0.8328 | Val Loss: 0.3736, Val Acc: 0.8479 | LR: 1.0000000000000004e-08
2025-02-10 17:51:26,625 INFO: Epoch [53/70] Train Loss: 0.3990, Train Acc: 0.8317 | Val Loss: 0.3738, Val Acc: 0.8496 | LR: 1.0000000000000004e-08
2025-02-10 17:52:22,436 INFO: Epoch [54/70] Train Loss: 0.3962, Train Acc: 0.8316 | Val Loss: 0.3764, Val Acc: 0.8519 | LR: 1.0000000000000004e-08
2025-02-10 17:53:17,637 INFO: Epoch [55/70] Train Loss: 0.3981, Train Acc: 0.8333 | Val Loss: 0.3676, Val Acc: 0.8544 | LR: 1.0000000000000004e-08
2025-02-10 17:54:14,267 INFO: Epoch [56/70] Train Loss: 0.3987, Train Acc: 0.8275 | Val Loss: 0.3753, Val Acc: 0.8475 | LR: 1.0000000000000004e-08
2025-02-10 17:55:10,756 INFO: Epoch [57/70] Train Loss: 0.3961, Train Acc: 0.8303 | Val Loss: 0.3649, Val Acc: 0.8500 | LR: 1.0000000000000004e-08
2025-02-10 17:55:10,760 INFO: New best model saved at epoch 57 with Val Loss: 0.3649
2025-02-10 17:56:07,367 INFO: Epoch [58/70] Train Loss: 0.3965, Train Acc: 0.8309 | Val Loss: 0.3673, Val Acc: 0.8552 | LR: 1.0000000000000004e-08
2025-02-10 17:57:02,805 INFO: Epoch [59/70] Train Loss: 0.3958, Train Acc: 0.8311 | Val Loss: 0.3724, Val Acc: 0.8477 | LR: 1.0000000000000004e-08
2025-02-10 17:57:58,028 INFO: Epoch [60/70] Train Loss: 0.3965, Train Acc: 0.8313 | Val Loss: 0.3710, Val Acc: 0.8494 | LR: 1.0000000000000004e-08
2025-02-10 17:58:53,319 INFO: Epoch [61/70] Train Loss: 0.4040, Train Acc: 0.8277 | Val Loss: 0.3697, Val Acc: 0.8525 | LR: 1.0000000000000004e-08
2025-02-10 17:59:50,393 INFO: Epoch [62/70] Train Loss: 0.3974, Train Acc: 0.8321 | Val Loss: 0.3761, Val Acc: 0.8469 | LR: 1.0000000000000004e-08
2025-02-10 18:00:45,752 INFO: Epoch [63/70] Train Loss: 0.3978, Train Acc: 0.8298 | Val Loss: 0.3664, Val Acc: 0.8556 | LR: 1.0000000000000004e-08
2025-02-10 18:01:41,333 INFO: Epoch [64/70] Train Loss: 0.4008, Train Acc: 0.8252 | Val Loss: 0.3691, Val Acc: 0.8486 | LR: 1.0000000000000004e-08
2025-02-10 18:02:36,774 INFO: Epoch [65/70] Train Loss: 0.3982, Train Acc: 0.8295 | Val Loss: 0.3748, Val Acc: 0.8477 | LR: 1.0000000000000004e-08
2025-02-10 18:03:32,354 INFO: Epoch [66/70] Train Loss: 0.3982, Train Acc: 0.8298 | Val Loss: 0.3687, Val Acc: 0.8511 | LR: 1.0000000000000004e-08
2025-02-10 18:04:27,692 INFO: Epoch [67/70] Train Loss: 0.3979, Train Acc: 0.8325 | Val Loss: 0.3715, Val Acc: 0.8502 | LR: 1.0000000000000004e-08
2025-02-10 18:05:22,847 INFO: Epoch [68/70] Train Loss: 0.3958, Train Acc: 0.8296 | Val Loss: 0.3661, Val Acc: 0.8506 | LR: 1.0000000000000004e-08
2025-02-10 18:06:18,308 INFO: Epoch [69/70] Train Loss: 0.3968, Train Acc: 0.8325 | Val Loss: 0.3698, Val Acc: 0.8523 | LR: 1.0000000000000004e-08
2025-02-10 18:07:13,583 INFO: Epoch [70/70] Train Loss: 0.3986, Train Acc: 0.8241 | Val Loss: 0.3664, Val Acc: 0.8511 | LR: 1.0000000000000004e-08
2025-02-10 18:07:13,589 INFO: Saved model and history to:
/media/Ittwin/E/_uni/DL_Project/tiger-fox-elephant/weights/elephant_with_augmentation_model.pth
/media/Ittwin/E/_uni/DL_Project/tiger-fox-elephant/weights/elephant_with_augmentation_history.npz
2025-02-11 12:41:23,980 INFO: Configuration:
2025-02-11 12:41:23,980 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 70
2025-02-11 12:41:23,981 INFO: Device: cuda
2025-02-11 12:41:50,963 WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.1793944..1.1413122].
