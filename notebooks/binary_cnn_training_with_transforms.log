2025-02-01 22:35:22,095 INFO: Configuration:
2025-02-01 22:35:22,095 INFO: IMG_SIZE: 224, BATCH_SIZE: 64, LR: 0.001, EPOCHS: 15
2025-02-01 22:35:22,096 INFO: Device: cuda
2025-02-01 22:35:22,212 INFO:  Target class: fox, Train samples: 9098, Val samples: 2600
2025-02-01 22:35:22,213 INFO: Data augmentation pipeline:
2025-02-01 22:35:22,213 INFO: ToPILImage()
2025-02-01 22:35:22,213 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 22:35:22,214 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 22:35:22,214 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 22:35:22,214 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 22:35:22,214 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 22:35:22,215 INFO: ToTensor()
2025-02-01 22:35:22,215 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 22:35:22,215 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 22:35:22,215 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 22:35:22,216 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 22:35:22,313 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 22:35:47,833 INFO: Epoch [1/15] Train Loss: 0.6762, Train Acc: 0.5841 | Val Loss: 0.6807, Val Acc: 0.5696 | LR: 0.001
2025-02-01 22:36:05,894 INFO: Configuration:
2025-02-01 22:36:05,895 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 15
2025-02-01 22:36:05,895 INFO: Device: cuda
2025-02-01 22:36:06,011 INFO:  Target class: fox, Train samples: 9098, Val samples: 2600
2025-02-01 22:36:06,011 INFO: Data augmentation pipeline:
2025-02-01 22:36:06,011 INFO: ToPILImage()
2025-02-01 22:36:06,012 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 22:36:06,012 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 22:36:06,012 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 22:36:06,012 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 22:36:06,012 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 22:36:06,012 INFO: ToTensor()
2025-02-01 22:36:06,012 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 22:36:06,013 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 22:36:06,013 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 22:36:06,013 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 22:36:06,167 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 22:36:32,130 INFO: Epoch [1/15] Train Loss: 0.6767, Train Acc: 0.5834 | Val Loss: 0.6731, Val Acc: 0.5727 | LR: 0.001
2025-02-01 22:36:57,476 INFO: Epoch [2/15] Train Loss: 0.6639, Train Acc: 0.6056 | Val Loss: 0.6657, Val Acc: 0.6019 | LR: 0.001
2025-02-01 22:37:22,797 INFO: Epoch [3/15] Train Loss: 0.6547, Train Acc: 0.6233 | Val Loss: 0.6522, Val Acc: 0.6169 | LR: 0.001
2025-02-01 22:37:47,966 INFO: Epoch [4/15] Train Loss: 0.6486, Train Acc: 0.6299 | Val Loss: 0.6463, Val Acc: 0.6392 | LR: 0.0001
2025-02-01 22:38:13,037 INFO: Epoch [5/15] Train Loss: 0.6428, Train Acc: 0.6394 | Val Loss: 0.6383, Val Acc: 0.6392 | LR: 0.0001
2025-02-01 22:38:38,550 INFO: Epoch [6/15] Train Loss: 0.6406, Train Acc: 0.6459 | Val Loss: 0.6414, Val Acc: 0.6338 | LR: 0.0001
2025-02-01 22:39:03,499 INFO: Epoch [7/15] Train Loss: 0.6373, Train Acc: 0.6496 | Val Loss: 0.6351, Val Acc: 0.6508 | LR: 1e-05
2025-02-01 22:44:02,372 INFO: Configuration:
2025-02-01 22:44:02,373 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 15
2025-02-01 22:44:02,373 INFO: Device: cuda
2025-02-01 22:44:02,488 INFO:  Target class: fox, Train samples: 9098, Val samples: 2600
2025-02-01 22:44:02,489 INFO: Data augmentation pipeline:
2025-02-01 22:44:02,489 INFO: ToPILImage()
2025-02-01 22:44:02,489 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 22:44:02,490 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 22:44:02,490 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 22:44:02,490 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 22:44:02,490 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 22:44:02,491 INFO: ToTensor()
2025-02-01 22:44:02,491 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 22:44:02,491 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 22:44:02,492 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 22:44:02,492 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 22:44:02,588 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 22:44:28,184 INFO: Epoch [1/15] Train Loss: 0.6767, Train Acc: 0.5834 | Val Loss: 0.6731, Val Acc: 0.5727 | LR: 0.001
2025-02-01 22:44:53,138 INFO: Epoch [2/15] Train Loss: 0.6639, Train Acc: 0.6056 | Val Loss: 0.6657, Val Acc: 0.6019 | LR: 0.001
2025-02-01 22:45:18,118 INFO: Epoch [3/15] Train Loss: 0.6547, Train Acc: 0.6233 | Val Loss: 0.6522, Val Acc: 0.6169 | LR: 0.001
2025-02-01 22:45:43,013 INFO: Epoch [4/15] Train Loss: 0.6495, Train Acc: 0.6310 | Val Loss: 0.6841, Val Acc: 0.5858 | LR: 0.001
2025-02-01 22:46:08,131 INFO: Epoch [5/15] Train Loss: 0.6393, Train Acc: 0.6457 | Val Loss: 0.6429, Val Acc: 0.6354 | LR: 0.001
2025-02-01 22:46:33,242 INFO: Epoch [6/15] Train Loss: 0.6322, Train Acc: 0.6525 | Val Loss: 0.6216, Val Acc: 0.6696 | LR: 0.001
2025-02-01 22:46:58,364 INFO: Epoch [7/15] Train Loss: 0.6192, Train Acc: 0.6741 | Val Loss: 0.6029, Val Acc: 0.6908 | LR: 0.001
2025-02-01 22:47:23,471 INFO: Epoch [8/15] Train Loss: 0.6070, Train Acc: 0.6818 | Val Loss: 0.5979, Val Acc: 0.6796 | LR: 0.001
2025-02-01 22:47:48,544 INFO: Epoch [9/15] Train Loss: 0.6009, Train Acc: 0.6862 | Val Loss: 0.6577, Val Acc: 0.6200 | LR: 0.001
2025-02-01 22:48:13,679 INFO: Epoch [10/15] Train Loss: 0.5862, Train Acc: 0.6940 | Val Loss: 0.5880, Val Acc: 0.6942 | LR: 0.001
2025-02-01 22:48:38,465 INFO: Epoch [11/15] Train Loss: 0.5839, Train Acc: 0.6915 | Val Loss: 0.6645, Val Acc: 0.6335 | LR: 0.001
2025-02-01 22:49:03,479 INFO: Epoch [12/15] Train Loss: 0.5756, Train Acc: 0.7076 | Val Loss: 0.5760, Val Acc: 0.7146 | LR: 0.001
2025-02-01 22:49:28,488 INFO: Epoch [13/15] Train Loss: 0.5671, Train Acc: 0.7120 | Val Loss: 0.5453, Val Acc: 0.7254 | LR: 0.001
2025-02-01 22:49:53,622 INFO: Epoch [14/15] Train Loss: 0.5588, Train Acc: 0.7202 | Val Loss: 0.5620, Val Acc: 0.7200 | LR: 0.001
2025-02-01 22:50:18,946 INFO: Epoch [15/15] Train Loss: 0.5513, Train Acc: 0.7250 | Val Loss: 0.5461, Val Acc: 0.7300 | LR: 0.001
2025-02-01 22:50:18,951 INFO: Saved model and history to:
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_model.pth
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_history.npz
2025-02-01 22:59:23,075 INFO: Configuration:
2025-02-01 22:59:23,076 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 15
2025-02-01 22:59:23,076 INFO: Device: cuda
2025-02-01 22:59:23,198 INFO:  Target class: fox, Train samples: 9098, Val samples: 2600
2025-02-01 22:59:23,199 INFO: Data augmentation pipeline:
2025-02-01 22:59:23,199 INFO: ToPILImage()
2025-02-01 22:59:23,199 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 22:59:23,199 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 22:59:23,199 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 22:59:23,200 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 22:59:23,200 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 22:59:23,200 INFO: ToTensor()
2025-02-01 22:59:23,200 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 22:59:23,200 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 22:59:23,200 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 22:59:23,200 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 22:59:45,690 INFO: Configuration:
2025-02-01 22:59:45,691 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 15
2025-02-01 22:59:45,691 INFO: Device: cuda
2025-02-01 22:59:45,808 INFO:  Target class: fox, Train samples: 9098, Val samples: 2600
2025-02-01 22:59:45,808 INFO: Data augmentation pipeline:
2025-02-01 22:59:45,809 INFO: ToPILImage()
2025-02-01 22:59:45,809 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 22:59:45,809 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 22:59:45,809 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 22:59:45,809 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 22:59:45,809 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 22:59:45,809 INFO: ToTensor()
2025-02-01 22:59:45,810 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 22:59:45,810 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 22:59:45,810 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 22:59:45,810 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 22:59:45,972 INFO: Loaded model from /media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_model.pth
2025-02-01 22:59:45,973 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 23:00:11,571 INFO: Epoch [1/15] Train Loss: 0.5420, Train Acc: 0.7311 | Val Loss: 0.5302, Val Acc: 0.7392 | LR: 0.001
2025-02-01 23:00:37,280 INFO: Epoch [2/15] Train Loss: 0.5384, Train Acc: 0.7320 | Val Loss: 0.5745, Val Acc: 0.7015 | LR: 0.001
2025-02-01 23:01:02,605 INFO: Epoch [3/15] Train Loss: 0.5323, Train Acc: 0.7406 | Val Loss: 0.5297, Val Acc: 0.7438 | LR: 0.001
2025-02-01 23:01:28,034 INFO: Epoch [4/15] Train Loss: 0.5343, Train Acc: 0.7386 | Val Loss: 0.5019, Val Acc: 0.7608 | LR: 0.001
2025-02-01 23:01:53,280 INFO: Epoch [5/15] Train Loss: 0.5253, Train Acc: 0.7439 | Val Loss: 0.5062, Val Acc: 0.7515 | LR: 0.001
2025-02-01 23:02:18,549 INFO: Epoch [6/15] Train Loss: 0.5267, Train Acc: 0.7437 | Val Loss: 0.4959, Val Acc: 0.7612 | LR: 0.001
2025-02-01 23:02:43,887 INFO: Epoch [7/15] Train Loss: 0.5281, Train Acc: 0.7458 | Val Loss: 0.5047, Val Acc: 0.7573 | LR: 0.001
2025-02-01 23:04:12,726 INFO: Configuration:
2025-02-01 23:04:12,727 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 15
2025-02-01 23:04:12,727 INFO: Device: cuda
2025-02-01 23:04:12,844 INFO:  Target class: fox, Train samples: 9098, Val samples: 2600
2025-02-01 23:04:12,845 INFO: Data augmentation pipeline:
2025-02-01 23:04:12,845 INFO: ToPILImage()
2025-02-01 23:04:12,845 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 23:04:12,846 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 23:04:12,846 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 23:04:12,846 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 23:04:12,847 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 23:04:12,847 INFO: ToTensor()
2025-02-01 23:04:12,847 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 23:04:12,847 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 23:04:12,848 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 23:04:12,848 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 23:04:12,947 INFO: Loaded model from /media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_model.pth
2025-02-01 23:04:12,947 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 23:04:38,790 INFO: Epoch [1/15] Train Loss: 0.5420, Train Acc: 0.7311 | Val Loss: 0.5302, Val Acc: 0.7392 | LR: 0.001
2025-02-01 23:04:38,795 INFO: New best model saved at epoch 1 with Val Loss: 0.5302
2025-02-01 23:05:04,412 INFO: Epoch [2/15] Train Loss: 0.5384, Train Acc: 0.7320 | Val Loss: 0.5745, Val Acc: 0.7015 | LR: 0.001
2025-02-01 23:05:29,682 INFO: Epoch [3/15] Train Loss: 0.5323, Train Acc: 0.7406 | Val Loss: 0.5297, Val Acc: 0.7438 | LR: 0.001
2025-02-01 23:05:29,817 INFO: New best model saved at epoch 3 with Val Loss: 0.5297
2025-02-01 23:05:55,408 INFO: Epoch [4/15] Train Loss: 0.5343, Train Acc: 0.7386 | Val Loss: 0.5019, Val Acc: 0.7608 | LR: 0.001
2025-02-01 23:05:55,554 INFO: New best model saved at epoch 4 with Val Loss: 0.5019
2025-02-01 23:06:20,806 INFO: Epoch [5/15] Train Loss: 0.5253, Train Acc: 0.7439 | Val Loss: 0.5062, Val Acc: 0.7515 | LR: 0.001
2025-02-01 23:06:46,190 INFO: Epoch [6/15] Train Loss: 0.5267, Train Acc: 0.7437 | Val Loss: 0.4959, Val Acc: 0.7612 | LR: 0.001
2025-02-01 23:06:46,326 INFO: New best model saved at epoch 6 with Val Loss: 0.4959
2025-02-01 23:07:11,633 INFO: Epoch [7/15] Train Loss: 0.5281, Train Acc: 0.7458 | Val Loss: 0.5047, Val Acc: 0.7573 | LR: 0.001
2025-02-01 23:07:37,035 INFO: Epoch [8/15] Train Loss: 0.5109, Train Acc: 0.7598 | Val Loss: 0.5710, Val Acc: 0.7115 | LR: 0.001
2025-02-01 23:08:02,537 INFO: Epoch [9/15] Train Loss: 0.5115, Train Acc: 0.7520 | Val Loss: 0.5662, Val Acc: 0.6973 | LR: 0.001
2025-02-01 23:08:27,875 INFO: Epoch [10/15] Train Loss: 0.5001, Train Acc: 0.7632 | Val Loss: 0.4990, Val Acc: 0.7615 | LR: 0.001
2025-02-01 23:08:53,420 INFO: Epoch [11/15] Train Loss: 0.4859, Train Acc: 0.7765 | Val Loss: 0.4647, Val Acc: 0.7869 | LR: 0.0001
2025-02-01 23:08:53,424 INFO: New best model saved at epoch 11 with Val Loss: 0.4647
2025-02-01 23:09:18,791 INFO: Epoch [12/15] Train Loss: 0.4749, Train Acc: 0.7807 | Val Loss: 0.4538, Val Acc: 0.7962 | LR: 0.0001
2025-02-01 23:09:18,794 INFO: New best model saved at epoch 12 with Val Loss: 0.4538
2025-02-01 23:09:44,153 INFO: Epoch [13/15] Train Loss: 0.4698, Train Acc: 0.7832 | Val Loss: 0.4499, Val Acc: 0.7946 | LR: 0.0001
2025-02-01 23:09:44,156 INFO: New best model saved at epoch 13 with Val Loss: 0.4499
2025-02-01 23:10:09,294 INFO: Epoch [14/15] Train Loss: 0.4671, Train Acc: 0.7872 | Val Loss: 0.4434, Val Acc: 0.8073 | LR: 0.0001
2025-02-01 23:10:09,298 INFO: New best model saved at epoch 14 with Val Loss: 0.4434
2025-02-01 23:10:34,447 INFO: Epoch [15/15] Train Loss: 0.4655, Train Acc: 0.7863 | Val Loss: 0.4453, Val Acc: 0.7942 | LR: 0.0001
2025-02-01 23:10:34,451 INFO: Saved model and history to:
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_model.pth
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/fox_with_augmentation_history.npz
2025-02-01 23:13:03,907 INFO: Configuration:
2025-02-01 23:13:03,908 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 30
2025-02-01 23:13:03,908 INFO: Device: cuda
2025-02-01 23:13:09,409 INFO:  Target class: tiger, Train samples: 9766, Val samples: 2790
2025-02-01 23:13:09,409 INFO: Data augmentation pipeline:
2025-02-01 23:13:09,410 INFO: ToPILImage()
2025-02-01 23:13:09,410 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 23:13:09,410 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 23:13:09,410 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 23:13:09,411 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 23:13:09,411 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 23:13:09,411 INFO: ToTensor()
2025-02-01 23:13:09,412 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 23:13:09,412 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 23:13:09,412 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 23:13:09,412 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 23:13:09,504 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 23:13:44,680 INFO: Configuration:
2025-02-01 23:13:44,680 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 30
2025-02-01 23:13:44,681 INFO: Device: cuda
2025-02-01 23:13:48,364 INFO:  Target class: tiger, Train samples: 9766, Val samples: 2790
2025-02-01 23:13:48,364 INFO: Data augmentation pipeline:
2025-02-01 23:13:48,365 INFO: ToPILImage()
2025-02-01 23:13:48,365 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 23:13:48,365 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 23:13:48,365 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 23:13:48,365 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 23:13:48,365 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 23:13:48,366 INFO: ToTensor()
2025-02-01 23:13:48,366 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 23:13:48,366 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 23:13:48,366 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 23:13:48,366 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 23:13:48,540 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 23:14:16,374 INFO: Epoch [1/30] Train Loss: 0.5938, Train Acc: 0.6723 | Val Loss: 0.5604, Val Acc: 0.7061 | LR: 0.001
2025-02-01 23:14:16,380 INFO: New best model saved at epoch 1 with Val Loss: 0.5604
2025-02-01 23:14:43,301 INFO: Epoch [2/30] Train Loss: 0.5604, Train Acc: 0.7111 | Val Loss: 0.5507, Val Acc: 0.7100 | LR: 0.001
2025-02-01 23:14:43,304 INFO: New best model saved at epoch 2 with Val Loss: 0.5507
2025-02-01 23:15:09,898 INFO: Epoch [3/30] Train Loss: 0.5506, Train Acc: 0.7225 | Val Loss: 0.5499, Val Acc: 0.7240 | LR: 0.001
2025-02-01 23:15:09,901 INFO: New best model saved at epoch 3 with Val Loss: 0.5499
2025-02-01 23:15:36,837 INFO: Epoch [4/30] Train Loss: 0.5301, Train Acc: 0.7378 | Val Loss: 0.6722, Val Acc: 0.6566 | LR: 0.001
2025-02-01 23:16:03,754 INFO: Epoch [5/30] Train Loss: 0.5229, Train Acc: 0.7406 | Val Loss: 0.5165, Val Acc: 0.7326 | LR: 0.001
2025-02-01 23:16:03,758 INFO: New best model saved at epoch 5 with Val Loss: 0.5165
2025-02-01 23:16:31,059 INFO: Epoch [6/30] Train Loss: 0.5124, Train Acc: 0.7472 | Val Loss: 0.5015, Val Acc: 0.7609 | LR: 0.001
2025-02-01 23:16:31,062 INFO: New best model saved at epoch 6 with Val Loss: 0.5015
2025-02-01 23:16:57,849 INFO: Epoch [7/30] Train Loss: 0.5083, Train Acc: 0.7535 | Val Loss: 0.4846, Val Acc: 0.7692 | LR: 0.001
2025-02-01 23:16:57,852 INFO: New best model saved at epoch 7 with Val Loss: 0.4846
2025-02-01 23:17:25,351 INFO: Epoch [8/30] Train Loss: 0.5006, Train Acc: 0.7624 | Val Loss: 0.4924, Val Acc: 0.7552 | LR: 0.001
2025-02-01 23:17:52,536 INFO: Epoch [9/30] Train Loss: 0.4865, Train Acc: 0.7727 | Val Loss: 0.6077, Val Acc: 0.6935 | LR: 0.001
2025-02-01 23:18:19,894 INFO: Epoch [10/30] Train Loss: 0.4773, Train Acc: 0.7741 | Val Loss: 0.4919, Val Acc: 0.7527 | LR: 0.001
2025-02-01 23:18:46,823 INFO: Epoch [11/30] Train Loss: 0.4670, Train Acc: 0.7823 | Val Loss: 0.4743, Val Acc: 0.7735 | LR: 0.001
2025-02-01 23:18:46,826 INFO: New best model saved at epoch 11 with Val Loss: 0.4743
2025-02-01 23:19:13,797 INFO: Epoch [12/30] Train Loss: 0.4682, Train Acc: 0.7865 | Val Loss: 0.5027, Val Acc: 0.7631 | LR: 0.001
2025-02-01 23:19:41,248 INFO: Epoch [13/30] Train Loss: 0.4538, Train Acc: 0.7933 | Val Loss: 0.4589, Val Acc: 0.7986 | LR: 0.001
2025-02-01 23:19:41,252 INFO: New best model saved at epoch 13 with Val Loss: 0.4589
2025-02-01 23:20:07,985 INFO: Epoch [14/30] Train Loss: 0.4536, Train Acc: 0.7913 | Val Loss: 0.4834, Val Acc: 0.7659 | LR: 0.001
2025-02-01 23:20:34,832 INFO: Epoch [15/30] Train Loss: 0.4466, Train Acc: 0.7987 | Val Loss: 0.5605, Val Acc: 0.7097 | LR: 0.001
2025-02-01 23:21:01,540 INFO: Epoch [16/30] Train Loss: 0.4383, Train Acc: 0.8015 | Val Loss: 0.4492, Val Acc: 0.7885 | LR: 0.001
2025-02-01 23:21:01,543 INFO: New best model saved at epoch 16 with Val Loss: 0.4492
2025-02-01 23:21:28,420 INFO: Epoch [17/30] Train Loss: 0.4320, Train Acc: 0.8040 | Val Loss: 0.6197, Val Acc: 0.6670 | LR: 0.001
2025-02-01 23:21:55,223 INFO: Epoch [18/30] Train Loss: 0.4328, Train Acc: 0.8065 | Val Loss: 0.4741, Val Acc: 0.7656 | LR: 0.001
2025-02-01 23:22:22,234 INFO: Epoch [19/30] Train Loss: 0.4241, Train Acc: 0.8110 | Val Loss: 0.5583, Val Acc: 0.7072 | LR: 0.001
2025-02-01 23:22:48,994 INFO: Epoch [20/30] Train Loss: 0.4189, Train Acc: 0.8124 | Val Loss: 0.3875, Val Acc: 0.8391 | LR: 0.001
2025-02-01 23:22:48,997 INFO: New best model saved at epoch 20 with Val Loss: 0.3875
2025-02-01 23:23:15,697 INFO: Epoch [21/30] Train Loss: 0.4106, Train Acc: 0.8207 | Val Loss: 0.4971, Val Acc: 0.7706 | LR: 0.001
2025-02-01 23:23:42,363 INFO: Epoch [22/30] Train Loss: 0.4029, Train Acc: 0.8245 | Val Loss: 0.4341, Val Acc: 0.8115 | LR: 0.001
2025-02-01 23:24:09,335 INFO: Epoch [23/30] Train Loss: 0.3987, Train Acc: 0.8247 | Val Loss: 0.5123, Val Acc: 0.7620 | LR: 0.001
2025-02-01 23:24:35,992 INFO: Epoch [24/30] Train Loss: 0.4013, Train Acc: 0.8230 | Val Loss: 0.4122, Val Acc: 0.8201 | LR: 0.001
2025-02-01 23:25:02,952 INFO: Epoch [25/30] Train Loss: 0.3754, Train Acc: 0.8414 | Val Loss: 0.3510, Val Acc: 0.8606 | LR: 0.0001
2025-02-01 23:25:02,956 INFO: New best model saved at epoch 25 with Val Loss: 0.3510
2025-02-01 23:25:29,646 INFO: Epoch [26/30] Train Loss: 0.3581, Train Acc: 0.8523 | Val Loss: 0.3686, Val Acc: 0.8337 | LR: 0.0001
2025-02-01 23:25:56,347 INFO: Epoch [27/30] Train Loss: 0.3636, Train Acc: 0.8517 | Val Loss: 0.3609, Val Acc: 0.8581 | LR: 0.0001
2025-02-01 23:26:22,934 INFO: Epoch [28/30] Train Loss: 0.3570, Train Acc: 0.8474 | Val Loss: 0.3795, Val Acc: 0.8269 | LR: 0.0001
2025-02-01 23:26:49,697 INFO: Epoch [29/30] Train Loss: 0.3582, Train Acc: 0.8493 | Val Loss: 0.3459, Val Acc: 0.8656 | LR: 0.0001
2025-02-01 23:26:49,702 INFO: New best model saved at epoch 29 with Val Loss: 0.3459
2025-02-01 23:27:16,711 INFO: Epoch [30/30] Train Loss: 0.3553, Train Acc: 0.8519 | Val Loss: 0.3523, Val Acc: 0.8559 | LR: 0.0001
2025-02-01 23:27:16,715 INFO: Saved model and history to:
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/tiger_with_augmentation_model.pth
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/tiger_with_augmentation_history.npz
2025-02-01 23:28:39,986 INFO: Configuration:
2025-02-01 23:28:39,986 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 30
2025-02-01 23:28:39,987 INFO: Device: cuda
2025-02-01 23:29:02,684 INFO:  Target class: elephant, Train samples: 16852, Val samples: 4814
2025-02-01 23:29:02,684 INFO: Data augmentation pipeline:
2025-02-01 23:29:02,685 INFO: ToPILImage()
2025-02-01 23:29:02,685 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 23:29:02,685 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 23:29:02,685 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 23:29:02,685 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 23:29:02,686 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 23:29:02,686 INFO: ToTensor()
2025-02-01 23:29:02,686 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 23:29:02,686 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 23:29:02,686 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 23:29:02,687 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 23:29:02,902 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 23:29:49,203 INFO: Epoch [1/30] Train Loss: 0.6659, Train Acc: 0.6045 | Val Loss: 0.6598, Val Acc: 0.6103 | LR: 0.001
2025-02-01 23:36:49,519 INFO: Configuration:
2025-02-01 23:36:49,519 INFO: IMG_SIZE: 224, BATCH_SIZE: 70, LR: 0.001, EPOCHS: 30
2025-02-01 23:36:49,520 INFO: Device: cuda
2025-02-01 23:36:54,992 INFO:  Target class: elephant, Train samples: 16852, Val samples: 4814
2025-02-01 23:36:54,993 INFO: Data augmentation pipeline:
2025-02-01 23:36:54,993 INFO: ToPILImage()
2025-02-01 23:36:54,993 INFO: RandomHorizontalFlip(p=0.5)
2025-02-01 23:36:54,993 INFO: RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)
2025-02-01 23:36:54,994 INFO: RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1), shear=[-5.0, 5.0])
2025-02-01 23:36:54,994 INFO: ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.05, 0.05))
2025-02-01 23:36:54,994 INFO: Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)
2025-02-01 23:36:54,994 INFO: ToTensor()
2025-02-01 23:36:54,994 INFO: RandomApply(
    p=0.2
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))
)
2025-02-01 23:36:54,995 INFO: RandomApply(
    p=0.2
    GaussianNoise(mean=0.0, std=0.05)
)
2025-02-01 23:36:54,995 INFO: RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)
2025-02-01 23:36:54,995 INFO: Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
2025-02-01 23:36:55,193 INFO: Model architecture:
ImprovedBinaryCNN(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): LeakyReLU(negative_slope=0.1)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act2): LeakyReLU(negative_slope=0.1)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act3): LeakyReLU(negative_slope=0.1)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2025-02-01 23:37:40,656 INFO: Epoch [1/30] Train Loss: 0.6659, Train Acc: 0.6045 | Val Loss: 0.6598, Val Acc: 0.6103 | LR: 0.001
2025-02-01 23:37:40,661 INFO: New best model saved at epoch 1 with Val Loss: 0.6598
2025-02-01 23:38:25,901 INFO: Epoch [2/30] Train Loss: 0.6376, Train Acc: 0.6421 | Val Loss: 0.6305, Val Acc: 0.6423 | LR: 0.001
2025-02-01 23:38:25,903 INFO: New best model saved at epoch 2 with Val Loss: 0.6305
2025-02-01 23:39:10,781 INFO: Epoch [3/30] Train Loss: 0.6176, Train Acc: 0.6647 | Val Loss: 0.6016, Val Acc: 0.6894 | LR: 0.001
2025-02-01 23:39:10,786 INFO: New best model saved at epoch 3 with Val Loss: 0.6016
2025-02-01 23:39:55,857 INFO: Epoch [4/30] Train Loss: 0.5938, Train Acc: 0.6920 | Val Loss: 0.5774, Val Acc: 0.7050 | LR: 0.001
2025-02-01 23:39:55,860 INFO: New best model saved at epoch 4 with Val Loss: 0.5774
2025-02-01 23:40:40,884 INFO: Epoch [5/30] Train Loss: 0.5741, Train Acc: 0.7050 | Val Loss: 0.5914, Val Acc: 0.6888 | LR: 0.001
2025-02-01 23:41:25,595 INFO: Epoch [6/30] Train Loss: 0.5664, Train Acc: 0.7137 | Val Loss: 0.5348, Val Acc: 0.7524 | LR: 0.001
2025-02-01 23:41:25,599 INFO: New best model saved at epoch 6 with Val Loss: 0.5348
2025-02-01 23:42:10,600 INFO: Epoch [7/30] Train Loss: 0.5502, Train Acc: 0.7298 | Val Loss: 0.5801, Val Acc: 0.6996 | LR: 0.001
2025-02-01 23:42:55,823 INFO: Epoch [8/30] Train Loss: 0.5333, Train Acc: 0.7407 | Val Loss: 0.9169, Val Acc: 0.5625 | LR: 0.001
2025-02-01 23:43:41,197 INFO: Epoch [9/30] Train Loss: 0.5278, Train Acc: 0.7403 | Val Loss: 0.5239, Val Acc: 0.7418 | LR: 0.001
2025-02-01 23:43:41,200 INFO: New best model saved at epoch 9 with Val Loss: 0.5239
2025-02-01 23:44:26,227 INFO: Epoch [10/30] Train Loss: 0.5298, Train Acc: 0.7467 | Val Loss: 0.5071, Val Acc: 0.7532 | LR: 0.001
2025-02-01 23:44:26,230 INFO: New best model saved at epoch 10 with Val Loss: 0.5071
2025-02-01 23:45:11,473 INFO: Epoch [11/30] Train Loss: 0.5060, Train Acc: 0.7587 | Val Loss: 0.4744, Val Acc: 0.7850 | LR: 0.001
2025-02-01 23:45:11,478 INFO: New best model saved at epoch 11 with Val Loss: 0.4744
2025-02-01 23:45:56,569 INFO: Epoch [12/30] Train Loss: 0.5077, Train Acc: 0.7585 | Val Loss: 0.4820, Val Acc: 0.7746 | LR: 0.001
2025-02-01 23:46:41,804 INFO: Epoch [13/30] Train Loss: 0.5002, Train Acc: 0.7673 | Val Loss: 0.5134, Val Acc: 0.7572 | LR: 0.001
2025-02-01 23:47:27,266 INFO: Epoch [14/30] Train Loss: 0.4910, Train Acc: 0.7694 | Val Loss: 0.5403, Val Acc: 0.7370 | LR: 0.001
2025-02-01 23:48:12,675 INFO: Epoch [15/30] Train Loss: 0.4874, Train Acc: 0.7712 | Val Loss: 0.4639, Val Acc: 0.7898 | LR: 0.001
2025-02-01 23:48:12,680 INFO: New best model saved at epoch 15 with Val Loss: 0.4639
2025-02-01 23:48:57,923 INFO: Epoch [16/30] Train Loss: 0.4883, Train Acc: 0.7716 | Val Loss: 0.4748, Val Acc: 0.7894 | LR: 0.001
2025-02-01 23:49:43,320 INFO: Epoch [17/30] Train Loss: 0.4753, Train Acc: 0.7798 | Val Loss: 0.4911, Val Acc: 0.7692 | LR: 0.001
2025-02-01 23:50:28,440 INFO: Epoch [18/30] Train Loss: 0.4809, Train Acc: 0.7783 | Val Loss: 0.4569, Val Acc: 0.8025 | LR: 0.001
2025-02-01 23:50:28,445 INFO: New best model saved at epoch 18 with Val Loss: 0.4569
2025-02-01 23:51:13,636 INFO: Epoch [19/30] Train Loss: 0.4718, Train Acc: 0.7801 | Val Loss: 0.4478, Val Acc: 0.7962 | LR: 0.001
2025-02-01 23:51:13,640 INFO: New best model saved at epoch 19 with Val Loss: 0.4478
2025-02-01 23:51:58,928 INFO: Epoch [20/30] Train Loss: 0.4708, Train Acc: 0.7849 | Val Loss: 0.5292, Val Acc: 0.7364 | LR: 0.001
2025-02-01 23:52:44,133 INFO: Epoch [21/30] Train Loss: 0.4654, Train Acc: 0.7863 | Val Loss: 0.5962, Val Acc: 0.7054 | LR: 0.001
2025-02-01 23:53:29,144 INFO: Epoch [22/30] Train Loss: 0.4666, Train Acc: 0.7859 | Val Loss: 0.4557, Val Acc: 0.7921 | LR: 0.001
2025-02-01 23:54:14,230 INFO: Epoch [23/30] Train Loss: 0.4575, Train Acc: 0.7917 | Val Loss: 0.4309, Val Acc: 0.8087 | LR: 0.001
2025-02-01 23:54:14,235 INFO: New best model saved at epoch 23 with Val Loss: 0.4309
2025-02-01 23:54:59,251 INFO: Epoch [24/30] Train Loss: 0.4566, Train Acc: 0.7945 | Val Loss: 0.4317, Val Acc: 0.8118 | LR: 0.001
2025-02-01 23:55:44,294 INFO: Epoch [25/30] Train Loss: 0.4481, Train Acc: 0.7965 | Val Loss: 0.4807, Val Acc: 0.7715 | LR: 0.001
2025-02-01 23:56:29,626 INFO: Epoch [26/30] Train Loss: 0.4485, Train Acc: 0.7954 | Val Loss: 0.4331, Val Acc: 0.8122 | LR: 0.001
2025-02-01 23:57:14,617 INFO: Epoch [27/30] Train Loss: 0.4386, Train Acc: 0.8050 | Val Loss: 0.4306, Val Acc: 0.8097 | LR: 0.001
2025-02-01 23:57:14,622 INFO: New best model saved at epoch 27 with Val Loss: 0.4306
2025-02-01 23:57:59,734 INFO: Epoch [28/30] Train Loss: 0.4389, Train Acc: 0.7998 | Val Loss: 0.4274, Val Acc: 0.8060 | LR: 0.001
2025-02-01 23:57:59,738 INFO: New best model saved at epoch 28 with Val Loss: 0.4274
2025-02-01 23:58:44,908 INFO: Epoch [29/30] Train Loss: 0.4383, Train Acc: 0.8040 | Val Loss: 0.4075, Val Acc: 0.8149 | LR: 0.001
2025-02-01 23:58:44,912 INFO: New best model saved at epoch 29 with Val Loss: 0.4075
2025-02-01 23:59:30,305 INFO: Epoch [30/30] Train Loss: 0.4298, Train Acc: 0.8117 | Val Loss: 0.4052, Val Acc: 0.8211 | LR: 0.001
2025-02-01 23:59:30,308 INFO: New best model saved at epoch 30 with Val Loss: 0.4052
2025-02-01 23:59:30,312 INFO: Saved model and history to:
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/elephant_with_augmentation_model.pth
/media/l1tw1n/E/_uni/DL_Project/tiger-fox-elephant/weights/elephant_with_augmentation_history.npz
